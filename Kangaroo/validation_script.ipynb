{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpus_bleu, SmoothingFunction\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# For ROUGE, either use 'rouge_score' or 'evaluate' (both shown below).\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2A) Using the 'rouge_score' library directly:\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2B) Or using Hugging Face's 'evaluate':\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# import evaluate\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# rouge_evaluator = evaluate.load('rouge')\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Make sure you have downloaded the NLTK tokenizer:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# For ROUGE, either use 'rouge_score' or 'evaluate' (both shown below).\n",
    "# 2A) Using the 'rouge_score' library directly:\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 2B) Or using Hugging Face's 'evaluate':\n",
    "# import evaluate\n",
    "# rouge_evaluator = evaluate.load('rouge')\n",
    "\n",
    "# Make sure you have downloaded the NLTK tokenizer:\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_bleu_and_rouge(csv_file_path):\n",
    "    \"\"\"\n",
    "    Reads the CSV with columns:\n",
    "        [Video_file, Q_ID, Prompt_name, Model_Name, Output, Cayla_Resp, Nami_Resp, Natalia_Resp]\n",
    "    Calculates aggregated BLEU and ROUGE across all rows.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Prepare lists for BLEU calculation\n",
    "    # BLEU expects:\n",
    "    #  - hypotheses as a list of tokenized strings\n",
    "    #  - list_of_references as a list of lists of tokenized references\n",
    "    hypotheses = []\n",
    "    list_of_references = []  # Each item is a list of references for that hypothesis\n",
    "\n",
    "    # Prepare ROUGE scorer\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # For storing cumulative ROUGE scores\n",
    "    total_rouge1 = 0.0\n",
    "    total_rouge2 = 0.0\n",
    "    total_rougeL = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Weâ€™ll iterate each row and gather references/hypothesis\n",
    "    for idx, row in df.iterrows():\n",
    "        model_output = row['Output']\n",
    "        ref1 = str(row['Cayla_Resp'])\n",
    "        ref2 = str(row['Nami_Resp'])\n",
    "        ref3 = str(row['Natalia_Resp'])\n",
    "\n",
    "        # Tokenize the hypothesis (model output) for BLEU\n",
    "        hypo_tokens = nltk.word_tokenize(model_output.lower())\n",
    "\n",
    "        # Tokenize each reference\n",
    "        ref_tokens_1 = nltk.word_tokenize(ref1.lower())\n",
    "        ref_tokens_2 = nltk.word_tokenize(ref2.lower())\n",
    "        ref_tokens_3 = nltk.word_tokenize(ref3.lower())\n",
    "\n",
    "        # For BLEU, we can have multiple references in a list:\n",
    "        references_for_this_row = []\n",
    "        # Only add reference if it's non-empty or not \"normal\"\n",
    "        # (You might need your own logic here if references can be empty or placeholders)\n",
    "        if len(ref_tokens_1) > 0:\n",
    "            references_for_this_row.append(ref_tokens_1)\n",
    "        if len(ref_tokens_2) > 0:\n",
    "            references_for_this_row.append(ref_tokens_2)\n",
    "        if len(ref_tokens_3) > 0:\n",
    "            references_for_this_row.append(ref_tokens_3)\n",
    "\n",
    "        # If for some reason all references are empty, skip (or handle differently)\n",
    "        if len(references_for_this_row) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add to BLEU lists\n",
    "        hypotheses.append(hypo_tokens)\n",
    "        list_of_references.append(references_for_this_row)\n",
    "\n",
    "        # For ROUGE, we typically provide a single hypothesis and multiple references.\n",
    "        # The 'rouge_score' library doesn't natively do \"multiple references\" at once,\n",
    "        # so you can average or take the max across references:\n",
    "        # We'll do a simple average of ROUGE scores across the references.\n",
    "        current_rouge1 = 0.0\n",
    "        current_rouge2 = 0.0\n",
    "        current_rougeL = 0.0\n",
    "\n",
    "        # Evaluate each reference individually and then average\n",
    "        for ref in [ref1, ref2, ref3]:\n",
    "            if ref.strip():\n",
    "                scores = rouge_scorer_obj.score(ref, model_output)\n",
    "                current_rouge1 += scores['rouge1'].fmeasure\n",
    "                current_rouge2 += scores['rouge2'].fmeasure\n",
    "                current_rougeL += scores['rougeL'].fmeasure\n",
    "\n",
    "        # Divide by number of non-empty references\n",
    "        non_empty_refs = sum([1 for r in [ref1, ref2, ref3] if r.strip()])\n",
    "        current_rouge1 /= non_empty_refs\n",
    "        current_rouge2 /= non_empty_refs\n",
    "        current_rougeL /= non_empty_refs\n",
    "\n",
    "        total_rouge1 += current_rouge1\n",
    "        total_rouge2 += current_rouge2\n",
    "        total_rougeL += current_rougeL\n",
    "        count += 1\n",
    "\n",
    "    # --- BLEU Calculation (corpus-level) ---\n",
    "    # Using a smoothing function is often recommended\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(list_of_references, hypotheses, smoothing_function=smoothie)\n",
    "\n",
    "    # --- ROUGE Calculation (averaged across rows) ---\n",
    "    if count > 0:\n",
    "        avg_rouge1 = total_rouge1 / count\n",
    "        avg_rouge2 = total_rouge2 / count\n",
    "        avg_rougeL = total_rougeL / count\n",
    "    else:\n",
    "        avg_rouge1 = avg_rouge2 = avg_rougeL = 0.0\n",
    "\n",
    "    print(\"===== BLEU & ROUGE Results =====\")\n",
    "    print(f\"BLEU (corpus-level): {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-1 (avg F-measure): {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-2 (avg F-measure): {avg_rouge2:.4f}\")\n",
    "    print(f\"ROUGE-L (avg F-measure): {avg_rougeL:.4f}\")\n",
    "\n",
    "    return bleu_score, avg_rouge1, avg_rouge2, avg_rougeL\n",
    "\n",
    "\n",
    "\n",
    "csv_file = pd.read_csv(\".//localdisk4/panwla/Projects/park_vlm/Kangaroo/kangaroo_results.csv\")\n",
    "calculate_bleu_and_rouge(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
