{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localdisk4/panwla/Projects/park_vlm/Kangaroo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.getcwd()\n",
    "os.chdir(\"/localdisk4/panwla/Projects/park_vlm/Kangaroo\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWV-LLyt56pv",
    "outputId": "d824fb64-dc1a-4006-a924-06b9a9a69e8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import yaml\n",
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive to access API keys\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Load API keys from file\n",
    "file_path = './.API_KEYS/API_KEYS.yml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    api_keys = yaml.safe_load(file)\n",
    "\n",
    "### WANDB Keys\n",
    "wandb_key = api_keys['WANDB']['Key']\n",
    "hf_read_api_key = api_keys['HUGGINGFACE']['HF_READ_API_KEY']\n",
    "\n",
    "# Extract AWS credentials\n",
    "aws_access_key_id = api_keys['AWS']['AWS_ACCESS_KEY_ID']\n",
    "aws_secret_access_key = api_keys['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(hf_read_api_key)\n",
    "\n",
    "# Initialize the S3 client with credentials\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables based on your bashrc settings\n",
    "os.environ[\"JUPYTER_DATA_DIR\"] = \"/localdisk4/panwla/jupyter_data\"\n",
    "os.environ[\"HF_HOME\"] = \"/localdisk4/panwla/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/localdisk4/panwla/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/localdisk4/panwla/huggingface_cache/datasets\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/localdisk4/panwla/torch\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/localdisk4/panwla_cache/.cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/localdisk4/panwla/tmp\"\n",
    "os.environ[\"TEMP\"] = \"/localdisk4/panwla/tmp\"\n",
    "os.environ[\"TMP\"] = \"/localdisk4/panwla/tmp\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "directories = [\n",
    "    os.environ[\"JUPYTER_DATA_DIR\"],\n",
    "    os.environ[\"HF_HOME\"],\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "    os.environ[\"HF_DATASETS_CACHE\"],\n",
    "    os.environ[\"TORCH_HOME\"],\n",
    "    os.environ[\"XDG_CACHE_HOME\"],\n",
    "    os.environ[\"TMPDIR\"],\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 72411.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint downloaded to: /localdisk4/panwla/huggingface_cache/models--KangarooGroup--kangaroo/snapshots/ded0c00d24ea319ca1cd549bb890dd577f3fed7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be imported now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/KangarooGroup/kangaroo:\n",
      "- modeling_kangaroo.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please 'pip install xformers'\n",
      "\n",
      "Model shifted to cuda successfully\n",
      "model.embed_tokens.weight is on cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.0.input_layernorm.weight is on cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.1.input_layernorm.weight is on cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.2.input_layernorm.weight is on cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.3.input_layernorm.weight is on cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.4.input_layernorm.weight is on cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.5.input_layernorm.weight is on cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.6.input_layernorm.weight is on cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.7.input_layernorm.weight is on cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.8.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.8.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.8.input_layernorm.weight is on cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.9.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.9.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.9.input_layernorm.weight is on cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.10.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.10.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.10.input_layernorm.weight is on cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.11.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.11.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.11.input_layernorm.weight is on cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.12.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.12.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.12.input_layernorm.weight is on cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.13.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.13.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.13.input_layernorm.weight is on cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.14.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.14.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.14.input_layernorm.weight is on cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.15.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.15.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.15.input_layernorm.weight is on cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.16.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.16.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.16.input_layernorm.weight is on cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.17.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.17.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.17.input_layernorm.weight is on cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.18.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.18.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.18.input_layernorm.weight is on cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.19.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.19.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.19.input_layernorm.weight is on cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.20.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.20.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.20.input_layernorm.weight is on cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.21.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.21.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.21.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.21.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.21.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.21.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.21.input_layernorm.weight is on cuda:0\n",
      "model.layers.21.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.22.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.22.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.22.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.22.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.22.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.22.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.22.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.22.input_layernorm.weight is on cuda:0\n",
      "model.layers.22.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.23.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.23.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.23.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.23.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.23.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.23.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.23.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.23.input_layernorm.weight is on cuda:0\n",
      "model.layers.23.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.24.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.24.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.24.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.24.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.24.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.24.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.24.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.24.input_layernorm.weight is on cuda:0\n",
      "model.layers.24.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.25.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.25.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.25.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.25.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.25.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.25.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.25.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.25.input_layernorm.weight is on cuda:0\n",
      "model.layers.25.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.26.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.26.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.26.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.26.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.26.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.26.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.26.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.26.input_layernorm.weight is on cuda:0\n",
      "model.layers.26.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.27.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.27.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.27.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.27.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.27.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.27.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.27.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.27.input_layernorm.weight is on cuda:0\n",
      "model.layers.27.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.28.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.28.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.28.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.28.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.28.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.28.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.28.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.28.input_layernorm.weight is on cuda:0\n",
      "model.layers.28.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.29.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.29.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.29.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.29.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.29.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.29.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.29.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.29.input_layernorm.weight is on cuda:0\n",
      "model.layers.29.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.30.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.30.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.30.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.30.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.30.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.30.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.30.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.30.input_layernorm.weight is on cuda:0\n",
      "model.layers.30.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.31.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.31.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.31.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.31.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.31.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.31.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.31.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.31.input_layernorm.weight is on cuda:0\n",
      "model.layers.31.post_attention_layernorm.weight is on cuda:0\n",
      "model.norm.weight is on cuda:0\n",
      "vision_tower.cls_token is on cuda:0\n",
      "vision_tower.pos_embed is on cuda:0\n",
      "vision_tower.patch_embed.proj.weight is on cuda:0\n",
      "vision_tower.patch_embed.proj.bias is on cuda:0\n",
      "vision_tower.blocks.0.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.0.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.0.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.0.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.0.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.0.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.0.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.0.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.0.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.0.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.0.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.0.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.0.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.0.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.0.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.0.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.0.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.0.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.0.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.0.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.0.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.1.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.1.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.1.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.1.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.1.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.1.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.1.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.1.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.1.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.1.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.1.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.1.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.1.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.1.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.1.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.1.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.1.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.1.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.1.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.1.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.1.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.2.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.2.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.2.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.2.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.2.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.2.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.2.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.2.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.2.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.2.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.2.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.2.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.2.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.2.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.2.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.2.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.2.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.2.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.2.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.2.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.2.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.3.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.3.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.3.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.3.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.3.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.3.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.3.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.3.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.3.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.3.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.3.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.3.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.3.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.3.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.3.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.3.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.3.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.3.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.3.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.3.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.3.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.4.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.4.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.4.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.4.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.4.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.4.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.4.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.4.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.4.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.4.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.4.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.4.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.4.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.4.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.4.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.4.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.4.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.4.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.4.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.4.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.4.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.5.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.5.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.5.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.5.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.5.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.5.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.5.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.5.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.5.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.5.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.5.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.5.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.5.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.5.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.5.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.5.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.5.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.5.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.5.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.5.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.5.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.6.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.6.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.6.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.6.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.6.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.6.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.6.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.6.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.6.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.6.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.6.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.6.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.6.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.6.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.6.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.6.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.6.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.6.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.6.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.6.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.6.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.7.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.7.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.7.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.7.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.7.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.7.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.7.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.7.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.7.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.7.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.7.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.7.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.7.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.7.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.7.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.7.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.7.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.7.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.7.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.7.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.7.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.8.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.8.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.8.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.8.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.8.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.8.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.8.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.8.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.8.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.8.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.8.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.8.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.8.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.8.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.8.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.8.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.8.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.8.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.8.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.8.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.8.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.9.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.9.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.9.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.9.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.9.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.9.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.9.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.9.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.9.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.9.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.9.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.9.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.9.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.9.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.9.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.9.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.9.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.9.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.9.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.9.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.9.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.10.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.10.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.10.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.10.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.10.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.10.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.10.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.10.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.10.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.10.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.10.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.10.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.10.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.10.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.10.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.10.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.10.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.10.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.10.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.10.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.10.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.11.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.11.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.11.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.11.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.11.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.11.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.11.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.11.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.11.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.11.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.11.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.11.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.11.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.11.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.11.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.11.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.11.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.11.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.11.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.11.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.11.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.12.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.12.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.12.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.12.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.12.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.12.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.12.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.12.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.12.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.12.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.12.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.12.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.12.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.12.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.12.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.12.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.12.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.12.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.12.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.12.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.12.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.13.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.13.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.13.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.13.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.13.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.13.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.13.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.13.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.13.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.13.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.13.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.13.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.13.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.13.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.13.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.13.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.13.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.13.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.13.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.13.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.13.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.14.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.14.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.14.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.14.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.14.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.14.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.14.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.14.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.14.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.14.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.14.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.14.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.14.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.14.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.14.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.14.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.14.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.14.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.14.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.14.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.14.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.15.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.15.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.15.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.15.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.15.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.15.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.15.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.15.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.15.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.15.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.15.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.15.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.15.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.15.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.15.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.15.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.15.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.15.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.15.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.15.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.15.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.16.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.16.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.16.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.16.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.16.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.16.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.16.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.16.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.16.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.16.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.16.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.16.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.16.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.16.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.16.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.16.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.16.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.16.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.16.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.16.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.16.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.17.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.17.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.17.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.17.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.17.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.17.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.17.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.17.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.17.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.17.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.17.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.17.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.17.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.17.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.17.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.17.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.17.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.17.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.17.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.17.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.17.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.18.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.18.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.18.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.18.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.18.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.18.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.18.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.18.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.18.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.18.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.18.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.18.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.18.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.18.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.18.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.18.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.18.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.18.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.18.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.18.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.18.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.19.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.19.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.19.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.19.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.19.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.19.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.19.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.19.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.19.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.19.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.19.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.19.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.19.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.19.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.19.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.19.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.19.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.19.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.19.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.19.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.19.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.20.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.20.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.20.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.20.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.20.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.20.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.20.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.20.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.20.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.20.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.20.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.20.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.20.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.20.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.20.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.20.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.20.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.20.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.20.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.20.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.20.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.21.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.21.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.21.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.21.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.21.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.21.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.21.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.21.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.21.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.21.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.21.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.21.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.21.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.21.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.21.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.21.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.21.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.21.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.21.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.21.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.21.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.22.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.22.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.22.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.22.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.22.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.22.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.22.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.22.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.22.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.22.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.22.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.22.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.22.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.22.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.22.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.22.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.22.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.22.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.22.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.22.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.22.mlp.w3.bias is on cuda:0\n",
      "vision_tower.blocks.23.norm1.weight is on cuda:0\n",
      "vision_tower.blocks.23.norm1.bias is on cuda:0\n",
      "vision_tower.blocks.23.attn.q_bias is on cuda:0\n",
      "vision_tower.blocks.23.attn.v_bias is on cuda:0\n",
      "vision_tower.blocks.23.attn.q_proj.weight is on cuda:0\n",
      "vision_tower.blocks.23.attn.k_proj.weight is on cuda:0\n",
      "vision_tower.blocks.23.attn.v_proj.weight is on cuda:0\n",
      "vision_tower.blocks.23.attn.inner_attn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.23.attn.inner_attn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.23.attn.proj.weight is on cuda:0\n",
      "vision_tower.blocks.23.attn.proj.bias is on cuda:0\n",
      "vision_tower.blocks.23.norm2.weight is on cuda:0\n",
      "vision_tower.blocks.23.norm2.bias is on cuda:0\n",
      "vision_tower.blocks.23.mlp.w1.weight is on cuda:0\n",
      "vision_tower.blocks.23.mlp.w1.bias is on cuda:0\n",
      "vision_tower.blocks.23.mlp.w2.weight is on cuda:0\n",
      "vision_tower.blocks.23.mlp.w2.bias is on cuda:0\n",
      "vision_tower.blocks.23.mlp.ffn_ln.weight is on cuda:0\n",
      "vision_tower.blocks.23.mlp.ffn_ln.bias is on cuda:0\n",
      "vision_tower.blocks.23.mlp.w3.weight is on cuda:0\n",
      "vision_tower.blocks.23.mlp.w3.bias is on cuda:0\n",
      "mm_projector.0.weight is on cuda:0\n",
      "mm_projector.0.bias is on cuda:0\n",
      "mm_projector.2.weight is on cuda:0\n",
      "mm_projector.2.bias is on cuda:0\n",
      "lm_head.weight is on cuda:0\n",
      "adaptive_pooling.weight is on cuda:0\n",
      "adaptive_pooling.bias is on cuda:0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Download the model checkpoint to a local directory\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Ensure HF_HOME & TMPDIR are set (if needed)\n",
    "os.environ[\"HF_HOME\"] = \"/localdisk4/panwla/huggingface_cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/localdisk4/panwla/tmp\"\n",
    "\n",
    "checkpoint_path = snapshot_download(\n",
    "    repo_id=\"KangarooGroup/kangaroo\",\n",
    "    cache_dir=\"/localdisk4/panwla/huggingface_cache\",\n",
    "    resume_download=True,\n",
    "    max_workers=1,  # reduce concurrency to avoid timeouts\n",
    ")\n",
    "\n",
    "print(\"Model checkpoint downloaded to:\", checkpoint_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KangarooGroup/kangaroo\")\n",
    "\n",
    "print(\"Model will be imported now\")\n",
    "\n",
    "# Option 1: Use a specific device (this worked before)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"KangarooGroup/kangaroo\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model shifted to cuda successfully\")\n",
    "\n",
    "# Print device mapping information\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on {param.device}\")\n",
    "\n",
    "# Define terminators\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def analyze_video(video_path, query):\n",
    "    \n",
    "    # Check if video file exists\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "    \n",
    "    # Define terminators (constant across all calls)\n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    \n",
    "    # Run the model's chat function with fixed parameters\n",
    "    output, _ = model.chat(\n",
    "        video_path=video_path,\n",
    "        query=query,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "facial_expressions_query = '''\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: Please describe whether the person demonstrates any difficulty through their facial expressions. Some examples of visible difficulty include furrowed brow, squinting eyes, clenched jaw, tight lips, head hanging low, sighing, wrinkled forehead, etc. Mention such specific details when found. End output with a final answer choice: \"Yes\" or \"No\".\n",
    "\n",
    "Answer:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Apparent_diff_task ='''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video. \n",
    "\n",
    "Question: Please describe whether the person demonstrates any difficulty through their facial expressions. Some examples of visible difficulty include furrowed brow, squinting eyes, clenched jaw, tight lips, head hanging low, sighing, wrinkled forehead, etc. Mention such specific details when found. End output with a final answer choice: “Yes” or “No”.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "BG_and_lighting_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: Mention if the background is overloaded (i.e., too many things), or the lighting condition is inappropriate (i.e., too dark or overlit). Otherwise, just output “normal background”.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Blink_rate_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: Was there anything abnormal about the person’s eye blink rate? For example, they may not be blinking at all, or they may have reduced or increased blink rate compared to a normal person. If there is nothing abnormal, output “normal blinking”.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Camera_position_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: How far is the camera? A good position of the camera would be when the upper half of the subject's body remains visible, while the lower half is not captured in the frame. If the upper body is only partly visible, the camera is too close. If the lower body is also visible, the camera is too far.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Coherence_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Analyze the provided text transcription of the person’s speech and answer the question about what is happening in the video.\n",
    "\n",
    "Transcription: <TRANSCRIPTION_OUTPUT>\n",
    "\n",
    "Question: Is the subject coherent in what they are speaking? Are they delivering an easy to understand story, or are they deterring a lot from the central topic and delivering an unorganized speech?\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Comp_task_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Analyze the provided text transcription of the person’s speech and answer the question about what is happening in the video.\n",
    "\n",
    "Transcription: <TRANSCRIPTION_OUTPUT>\n",
    "\n",
    "Question: Please indicate whether the subject was able to follow the instructions while completing the task. If the subject was doing something differently, please describe.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Lip_parting_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate the extent of lips parting when the subject is not saying anything (i.e., always/most of the times/sometimes/very few times/never).\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Masked_faceExp_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate which of the following are true for the subject: (i) The individual's face appears blank and emotionless, even when they are trying to express an emotion. (ii) The expression is weak or asymmetrical, and the individual has difficulty holding an expression (e.g., smile) for an extended period. Also mention if you observe other facial expression abnormalities.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "Observations_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Document any abnormal signs observed in body parts other than the face. This includes, but is not limited to, tremors in the hands, involuntary shaking or rhythmic movements of the upper or lower extremities, stiffness or rigidity in the limbs, reduced arm swing while speaking, or any signs of bradykinesia (slowness of movement). Additionally, note any abnormal postures, difficulty in maintaining balance, or other motor irregularities that may be indicative of Parkinson’s disease.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Overall_app_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Provide a brief description of the subject's perceived state of mind, noting whether they appear energetic, exhausted, calm, confused, or exhibit any other relevant emotional or cognitive cues.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Other_people_present_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate whether any other individuals were present in the background. If so, provide a brief description (e.g., \"An older male is visible in the background\"). Conclude with a final answer: \"Yes\" or \"No\".\n",
    "\n",
    "Answer: \n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Comp_sent_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Analyze the provided text transcription of the person’s speech and answer the question about what is happening in the video.\n",
    "\n",
    "Transcription: <TRANSCRIPTION_OUTPUT>\n",
    "\n",
    "\n",
    "Question: Indicate whether the subject is using complex sentences that are difficult to understand. Conclude with a final answer: \"Yes\" or \"No\".\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Other_BP_visible_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate if any body part critical for this task is partially visible (or invisible). For example, was the subject wearing a mask that may have obstructed important visual information? Or did the face go out of frame while the subject was completing the task?\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === Model inference function (as provided) ===\n",
    "def analyze_video(video_path, query):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "\n",
    "    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "    output, _ = model.chat(\n",
    "        video_path=video_path,\n",
    "        query=query,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Setup file paths ===\n",
    "cayla_files = \"/localdisk4/panwla/Projects/park_vlm/Annotations/Clinical/Cayla/cayla_common_final.csv\"\n",
    "nami_files = \"/localdisk4/panwla/Projects/park_vlm/Annotations/Clinical/Nami/nami_common_final.csv\"\n",
    "natalia_files = \"/localdisk4/panwla/Projects/park_vlm/Annotations/Clinical/Natalia/natalia_common_final.csv\"\n",
    "\n",
    "video_path = \"/localdisk4/panwla/Projects/park_vlm/Videos/Videos\"\n",
    "\n",
    "# === Load video references ===\n",
    "cayla_df = pd.read_csv(cayla_files)\n",
    "nami_df = pd.read_csv(nami_files)\n",
    "natalia_df = pd.read_csv(natalia_files)\n",
    "\n",
    "# List of column indices to select\n",
    "columns_to_keep = [0, 1, 2, 3, 5, 6, 8, 14, 18, 21, 24, 28, 29, -1]\n",
    "\n",
    "# Subset Cayla\n",
    "cayla_df = cayla_df.iloc[:, columns_to_keep]\n",
    "\n",
    "# Identify prompt + video columns\n",
    "prompt_columns = list(cayla_df.columns[0:13])  # first 13 columns\n",
    "video_column = ['video']\n",
    "\n",
    "# Subset Nami and Natalia using same columns\n",
    "nami_df = nami_df.loc[:, prompt_columns + video_column]\n",
    "natalia_df = natalia_df.loc[:, prompt_columns + video_column]\n",
    "\n",
    "# List available videos\n",
    "available_videos = os.listdir(video_path)\n",
    "\n",
    "# === Empty DataFrame to collect results ===\n",
    "columns = [\"Video_file\", \"Q_ID\", \"Prompt_name\", \"Model_Name\", \"Cayla_Resp\", \"Nami_Resp\", \"Natalia_Resp\"]\n",
    "output = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# === Store prompts in a dictionary ===\n",
    "prompt_dict = {\n",
    "    \"facial_expressions_query\": facial_expressions_query,\n",
    "    \"BG_and_lighting_query\": BG_and_lighting_query,\n",
    "    \"Blink_rate_query\": Blink_rate_query,\n",
    "    \"Camera_position_query\": Camera_position_query,\n",
    "    \"Coherence_query\": Coherence_query,\n",
    "    \"Comp_task_query\": Comp_task_query,\n",
    "    \"Lip_parting_query\": Lip_parting_query,\n",
    "    \"Masked_faceExp_query\": Masked_faceExp_query,\n",
    "    \"Observations_query\": Observations_query,\n",
    "    \"Overall_app_query\": Overall_app_query,\n",
    "    \"Other_people_present_query\": Other_people_present_query,\n",
    "    \"Other_BP_visible_query\": Other_BP_visible_query,\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apparent difficulty completing the task</th>\n",
       "      <th>Background and lighting</th>\n",
       "      <th>Blink rate</th>\n",
       "      <th>Camera Position</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Compliance with tasks instructions</th>\n",
       "      <th>Lips parting when the mouth is at rest</th>\n",
       "      <th>Masked facies</th>\n",
       "      <th>Observations of other body parts not being directly assessed</th>\n",
       "      <th>Overall appearance</th>\n",
       "      <th>Presence of other persons</th>\n",
       "      <th>Usage of complex sentence</th>\n",
       "      <th>Visibility of significant body parts</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>normal background</td>\n",
       "      <td>reduced</td>\n",
       "      <td>too close</td>\n",
       "      <td>coherent</td>\n",
       "      <td>Yes, follows instructions</td>\n",
       "      <td>very few times</td>\n",
       "      <td>neither</td>\n",
       "      <td>cannot view other body parts</td>\n",
       "      <td>anxious</td>\n",
       "      <td>none</td>\n",
       "      <td>simple sentences</td>\n",
       "      <td>normal</td>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>none</td>\n",
       "      <td>normal background</td>\n",
       "      <td>normal</td>\n",
       "      <td>too close</td>\n",
       "      <td>coherent</td>\n",
       "      <td>Yes, followed instructions</td>\n",
       "      <td>never</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>calm</td>\n",
       "      <td>there is another person partially visible on t...</td>\n",
       "      <td>easy to understand</td>\n",
       "      <td>adequate</td>\n",
       "      <td>2023-05-19T19%3A18%3A06.618Z_tV9SXnOdS3hUx6RXN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Apparent difficulty completing the task Background and lighting Blink rate  \\\n",
       "0                                    none       normal background    reduced   \n",
       "1                                    none       normal background     normal   \n",
       "\n",
       "  Camera Position Coherence Compliance with tasks instructions  \\\n",
       "0       too close  coherent          Yes, follows instructions   \n",
       "1       too close  coherent         Yes, followed instructions   \n",
       "\n",
       "  Lips parting when the mouth is at rest Masked facies  \\\n",
       "0                         very few times       neither   \n",
       "1                                  never          none   \n",
       "\n",
       "  Observations of other body parts not being directly assessed  \\\n",
       "0                       cannot view other body parts             \n",
       "1                                               none             \n",
       "\n",
       "  Overall appearance                          Presence of other persons  \\\n",
       "0            anxious                                               none   \n",
       "1               calm  there is another person partially visible on t...   \n",
       "\n",
       "  Usage of complex sentence Visibility of significant body parts  \\\n",
       "0          simple sentences                               normal   \n",
       "1        easy to understand                             adequate   \n",
       "\n",
       "                                               video  \n",
       "0  2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...  \n",
       "1  2023-05-19T19%3A18%3A06.618Z_tV9SXnOdS3hUx6RXN...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cayla_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apparent difficulty completing the task</th>\n",
       "      <th>Background and lighting</th>\n",
       "      <th>Blink rate</th>\n",
       "      <th>Camera Position</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Compliance with tasks instructions</th>\n",
       "      <th>Lips parting when the mouth is at rest</th>\n",
       "      <th>Masked facies</th>\n",
       "      <th>Observations of other body parts not being directly assessed</th>\n",
       "      <th>Overall appearance</th>\n",
       "      <th>Presence of other persons</th>\n",
       "      <th>Usage of complex sentence</th>\n",
       "      <th>Visibility of significant body parts</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>normal background</td>\n",
       "      <td>reduced</td>\n",
       "      <td>too close</td>\n",
       "      <td>coherent</td>\n",
       "      <td>Yes, follows instructions</td>\n",
       "      <td>very few times</td>\n",
       "      <td>neither</td>\n",
       "      <td>cannot view other body parts</td>\n",
       "      <td>anxious</td>\n",
       "      <td>none</td>\n",
       "      <td>simple sentences</td>\n",
       "      <td>normal</td>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Apparent difficulty completing the task Background and lighting Blink rate  \\\n",
       "0                                    none       normal background    reduced   \n",
       "\n",
       "  Camera Position Coherence Compliance with tasks instructions  \\\n",
       "0       too close  coherent          Yes, follows instructions   \n",
       "\n",
       "  Lips parting when the mouth is at rest Masked facies  \\\n",
       "0                         very few times       neither   \n",
       "\n",
       "  Observations of other body parts not being directly assessed  \\\n",
       "0                       cannot view other body parts             \n",
       "\n",
       "  Overall appearance Presence of other persons Usage of complex sentence  \\\n",
       "0            anxious                      none          simple sentences   \n",
       "\n",
       "  Visibility of significant body parts  \\\n",
       "0                               normal   \n",
       "\n",
       "                                               video  \n",
       "0  2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cayla_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apparent difficulty completing the task</th>\n",
       "      <th>Background and lighting</th>\n",
       "      <th>Blink rate</th>\n",
       "      <th>Camera Position</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Compliance with tasks instructions</th>\n",
       "      <th>Lips parting when the mouth is at rest</th>\n",
       "      <th>Masked facies</th>\n",
       "      <th>Observations of other body parts not being directly assessed</th>\n",
       "      <th>Overall appearance</th>\n",
       "      <th>Presence of other persons</th>\n",
       "      <th>Usage of complex sentence</th>\n",
       "      <th>Visibility of significant body parts</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>normal background</td>\n",
       "      <td>Blink rate was reduced.</td>\n",
       "      <td>good position</td>\n",
       "      <td>normal coherence, easy to understand story</td>\n",
       "      <td>Yes, she completes the task appropriately.</td>\n",
       "      <td>very few times (only able to assess for a shor...</td>\n",
       "      <td>The individual’s face appears emotionless</td>\n",
       "      <td>none noted</td>\n",
       "      <td>calm</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no problems</td>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Apparent difficulty completing the task Background and lighting  \\\n",
       "0                                      no       normal background   \n",
       "\n",
       "                 Blink rate Camera Position  \\\n",
       "0  Blink rate was reduced.    good position   \n",
       "\n",
       "                                    Coherence  \\\n",
       "0  normal coherence, easy to understand story   \n",
       "\n",
       "            Compliance with tasks instructions  \\\n",
       "0  Yes, she completes the task appropriately.    \n",
       "\n",
       "              Lips parting when the mouth is at rest  \\\n",
       "0  very few times (only able to assess for a shor...   \n",
       "\n",
       "                               Masked facies  \\\n",
       "0  The individual’s face appears emotionless   \n",
       "\n",
       "  Observations of other body parts not being directly assessed  \\\n",
       "0                                         none noted             \n",
       "\n",
       "  Overall appearance Presence of other persons Usage of complex sentence  \\\n",
       "0               calm                        no                        no   \n",
       "\n",
       "  Visibility of significant body parts  \\\n",
       "0                          no problems   \n",
       "\n",
       "                                               video  \n",
       "0  2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "natalia_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Apparent difficulty completing the task</th>\n",
       "      <th>Background and lighting</th>\n",
       "      <th>Blink rate</th>\n",
       "      <th>Camera Position</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Compliance with tasks instructions</th>\n",
       "      <th>Lips parting when the mouth is at rest</th>\n",
       "      <th>Masked facies</th>\n",
       "      <th>Observations of other body parts not being directly assessed</th>\n",
       "      <th>Overall appearance</th>\n",
       "      <th>Presence of other persons</th>\n",
       "      <th>Usage of complex sentence</th>\n",
       "      <th>Visibility of significant body parts</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>none</td>\n",
       "      <td>normal background</td>\n",
       "      <td>minimally decreased at times</td>\n",
       "      <td>good</td>\n",
       "      <td>coherence</td>\n",
       "      <td>compliant</td>\n",
       "      <td>unable to determine, only at rest for 1 sec</td>\n",
       "      <td>less spontaneous lower facial expression</td>\n",
       "      <td>none</td>\n",
       "      <td>calm</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>good, face visible</td>\n",
       "      <td>2023-09-14T16%3A04%3A58.255Z_G7Q1bZD2LUQN0v3Ll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Apparent difficulty completing the task Background and lighting  \\\n",
       "0                                    none       normal background   \n",
       "\n",
       "                     Blink rate Camera Position  Coherence  \\\n",
       "0  minimally decreased at times            good  coherence   \n",
       "\n",
       "  Compliance with tasks instructions  \\\n",
       "0                          compliant   \n",
       "\n",
       "        Lips parting when the mouth is at rest  \\\n",
       "0  unable to determine, only at rest for 1 sec   \n",
       "\n",
       "                              Masked facies  \\\n",
       "0  less spontaneous lower facial expression   \n",
       "\n",
       "  Observations of other body parts not being directly assessed  \\\n",
       "0                                               none             \n",
       "\n",
       "  Overall appearance Presence of other persons Usage of complex sentence  \\\n",
       "0               calm                      none                      none   \n",
       "\n",
       "  Visibility of significant body parts  \\\n",
       "0                   good, face visible   \n",
       "\n",
       "                                               video  \n",
       "0  2023-09-14T16%3A04%3A58.255Z_G7Q1bZD2LUQN0v3Ll...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nami_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 14), (25, 14), (25, 14))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cayla_df.shape, nami_df.shape, natalia_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Apparent difficulty completing the task', 'Background and lighting',\n",
       "       'Blink rate', 'Camera Position', 'Coherence',\n",
       "       'Compliance with tasks instructions',\n",
       "       'Lips parting when the mouth is at rest', 'Masked facies',\n",
       "       'Observations of other body parts not being directly assessed',\n",
       "       'Overall appearance', 'Presence of other persons',\n",
       "       'Usage of complex sentence', 'Visibility of significant body parts',\n",
       "       'video'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nami_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "set(prompt_columns)- set(nami_df[prompt_columns].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), set(), set())"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "set(prompt_columns)- set(cayla_df.columns), set(prompt_columns)- set(nami_df.columns), set(prompt_columns)- set(natalia_df.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cayla_df['video'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'openai/whisper-large-v3' on device 'cuda', dtype=torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "############### Load Whishper Model #########\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Optional: select your device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "torch_dtype = torch.float16 if \"cuda\" in device else torch.float32\n",
    "\n",
    "print(f\"Loading model '{model_id}' on device '{device}', dtype={torch_dtype}\")\n",
    "whishper_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "whishper_model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create the pipeline just once\n",
    "pipe_asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=whishper_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=0 if device.startswith(\"cuda\") else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 692 videos in directory\n",
      "\n",
      "🎬 Found match for row 0: 2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4\n",
      "📝 Transcription:  The latest series I just watched was Firefly Lane. It's about two girls growing up, best friends, have had their share of arguments, fights, whatever, and shows them all through their whole life.\n",
      "\n",
      "🎬 Found match for row 1: 2023-05-19T19%3A18%3A06.618Z_tV9SXnOdS3hUx6RXNvFDrCN6pHr2_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-05-19T19%3A18%3A06.618Z_tV9SXnOdS3hUx6RXNvFDrCN6pHr2_resting_face.mp4\n",
      "📝 Transcription:  I recently read a book called Tomorrow and Tomorrow and Tomorrow, which is really about a book about cultural differences in individuals, but it's demonstrated through the design of video games through an MIT and Harvard students who have gotten together prior in their lives and then met up subsequently at these two very prestigious schools. They move back and forth in the book between the East and West Coast because they originally came from the West Coast and one of them is of Asian descent, the other is biracial. And so meeting all their friends and experiencing their lives was very interesting to me and understanding actually about the creation of video games, which I thought was quite interesting since I didn't know anything about it.\n",
      "\n",
      "🎬 Found match for row 2: 2023-05-25T16%3A37%3A17.423Z_Anh9INYBNLRlano3pPxCOgFro3V2_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-05-25T16%3A37%3A17.423Z_Anh9INYBNLRlano3pPxCOgFro3V2_resting_face.mp4\n",
      "📝 Transcription:  Now you can go ahead with the... Okay, so the most recent movie that I watched was an oldie but goodie. We watched it over the weekend. It's called Parent Trap, and it stars Hayley Mills. I saw this as I was a child. It's about her parent, these two twins that have parents that split them up when they divorced years ago. And they never knew they had a twin sibling. And so one girl went to the father and one girl went to the mother. And they lived in opposite states. And they met at camp. And they got together and tried to figure out a way to get their parents back together. And so that's what the story was about.\n",
      "\n",
      "🎬 Found match for row 3: 2023-05-25T17%3A04%3A42.075Z_tw4W41JJxdgW6Knaehd5sz4VByb2_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-05-25T17%3A04%3A42.075Z_tw4W41JJxdgW6Knaehd5sz4VByb2_resting_face.mp4\n",
      "📝 Transcription:  Now you can talk. Ending Parkinson's by Dr. Dorsey. You want to talk a little bit about what was in the book? It talked mainly about the use of pesticides and how the use of pesticides in the United States is causing a Parkinson's pandemic. All right. Go ahead and press finish.\n",
      "\n",
      "🎬 Found match for row 4: 2023-06-27T19%3A52%3A53.484Z_Fy4vQjZMAiO1rGGxw9Nr2Vxj5Jn1_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-06-27T19%3A52%3A53.484Z_Fy4vQjZMAiO1rGGxw9Nr2Vxj5Jn1_resting_face.mp4\n",
      "📝 Transcription:  I've been watching the show and Just Like That, the reboot of Sex and the City and it's marginal at best. How long do I have to talk? You have to talk really long. Well, I can't stand Cynthia Nixon. I do like Carrie Bradshaw, Sarah Jessica Parker and the other ones I'm eh ambivalent about. Did just finish The Queen or Queen Charlotte which I really enjoyed.\n",
      "\n",
      "🎬 Found match for row 5: 2023-07-20T20%3A18%3A13.617Z_RljIq5LKP8OSq4TiRrdTwyS3UNm1_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-07-20T20%3A18%3A13.617Z_RljIq5LKP8OSq4TiRrdTwyS3UNm1_resting_face.mp4\n",
      "📝 Transcription:  The most recent movie I watched was All's Quiet on the Western Front. It's based on a book that was written taking a look at World War I and from the perspective of the German soldiers and not the American soldiers and what they went through and that transition that they made. And first being so excited about going to war, somewhat similar to the book Red Badge of Courage, and then realizing what war really is all about. How dark, how evil, how unfair, how noisy, how disgusting, how you change yourself and your belief system in a very short period of time just to stay alive. you\n",
      "\n",
      "🎬 Found match for row 6: 2024-03-01T17%3A53%3A33.326Z_G7Q1bZD2LUQN0v3LlvyBMzoTWic2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-03-01T17%3A53%3A33.326Z_G7Q1bZD2LUQN0v3LlvyBMzoTWic2_speech.mp4\n",
      "📝 Transcription:  The book that I recently read was called Oath and Honor by Liz Cheney. It was her account of what happened on January 6th, the insurrection, and then going through with the committee to investigate the January 6th attack. It was very scary to read because there were some congresspeople that didn't seem to be too bothered with people right outside their door trying to break it down and do them harm. But on the other hand, it really showed who was, which Congress people were concerned about keeping our democracy and others that weren't. There were some that just said, winning is everything. And it was a very good book. I think everybody in America should read it. It really does say what could happen if we lost our democracy.\n",
      "\n",
      "🎬 Found match for row 7: 2024-04-17T13%3A49%3A53.597Z_s4whCPQSZRb0rbGW2ye9dO4i7SV2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-04-17T13%3A49%3A53.597Z_s4whCPQSZRb0rbGW2ye9dO4i7SV2_speech.mp4\n",
      "📝 Transcription:  I recently watched the last episode of Curb Your Enthusiasm, which has been going on for 12 years or more, even though not every year. And Larry David did not disappoint. It was very funny. And about a third of the way through it, I said, oh, this is where he's going. He's going to end up back exactly how he did the last Seinfeld. and sure enough that's what he did ended up in jail talking about the first thing that the series started with so um it was uh i i thought it wasn't bad on seinfeld when that happened and uh i i thought it was a nice uh it's the way larry would do things knowing him um and uh so he he went back to where he was and uh started at the beginning but Did a nice little twist to counter all the disappointed fans who weren't happy with the last side photo.\n",
      "\n",
      "🎬 Found match for row 8: 2023-11-29T13%3A55%3A59.314Z_DNoEpcphLLNRAvkmByusfTMDrcf1_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-11-29T13%3A55%3A59.314Z_DNoEpcphLLNRAvkmByusfTMDrcf1_speech.mp4\n",
      "📝 Transcription:  It's Christmas time and one of my favorite movies is It's a Wonderful Life where this individual tries to commit suicide because he thinks he's messed up everyone and had terrible life but then an angel comes and jumps in the water and George jumps in the water to save the angel thus saving himself and the angel grants him a wish where he can see what life would be if he was never born he finds out that if he was never born the world would be so much different and worse and in the end he finds out it's so much better with him in it and that's the miracle have gratitude and appreciate what you have and don't think about what you don't have\n",
      "\n",
      "🎬 Found match for row 9: 2024-04-10T13%3A08%3A30.436Z_HsIl9SZejHd1U8Nx7nrGISMU7pN2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-04-10T13%3A08%3A30.436Z_HsIl9SZejHd1U8Nx7nrGISMU7pN2_speech.mp4\n",
      "📝 Transcription:  The last book that I watched is called First Lie Wins. It's about a girl who starts stealing because it was just her and her mom. And she started stealing to make money for them and for food, basically. And somehow she got caught one time and ended up working for this guy that caught her. so she was doing different jobs for him she never met him or never talked um didn't really talk to him much either he would just give her the job assignment she would go do it and then she would take on a different identity in each of the jobs um she then had a job with um this other guy was was uh the person she was trying to steal from and um he ended up was kind of in on it also\n",
      "\n",
      "🎬 Found match for row 10: 2024-08-07T17%3A01%3A23.090Z_WyZBRwAbSqh6LgPieLIhj2wfblq2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-08-07T17%3A01%3A23.090Z_WyZBRwAbSqh6LgPieLIhj2wfblq2_speech.mp4\n",
      "📝 Transcription:  I am currently reading a book called Demon Copperhead by Barbara Kingsolver. She also wrote the Poisonwood Bible that I read many years ago. Anyway, it's a book about a young boy growing up in foster care with lots and lots of issues. I think it takes place in West Virginia or in Appalachia somewhere. And it's a story of his misfortunes for many of his years and the death of both his parents. I haven't yet finished the book, but it's going into the, what am I trying to say? All of the problems with the opioid addictions in that part of the country, which is quite frightening. and it involves a lot of his friends and some of his talents. He's very artistic. And so I think in the end, this is going to play a, hopefully a very positive part to the ending of the story.\n",
      "\n",
      "🎬 Found match for row 11: 2023-06-04T19%3A28%3A53.508Z_e7GyfN6FRnbbYm5ClY01rXFbOW53_resting_face.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-06-04T19%3A28%3A53.508Z_e7GyfN6FRnbbYm5ClY01rXFbOW53_resting_face.mp4\n",
      "📝 Transcription:  The latest movie that I watched on my TV was Air Jordan, which was about Michael Jordan being recruited by Nike in order to represent them with his athletic footwear. And it was kind of a very interesting movie about negotiations that Nike had with Michael Jordan. and there was some seriousness along with some comedy in this show. And what happened was they ended up signing Michael Jordan and he was negotiated. His mother did the negotiations. And not only did he get signed up for $250,000, but because of his agreement with them, He was also able to make $400 million that first year based on the selling of.\n",
      "\n",
      "🎬 Found match for row 12: 2024-07-22T20%3A07%3A49.541Z_xyOmupmKagNJi5TW3uxpRJYdDwg2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-07-22T20%3A07%3A49.541Z_xyOmupmKagNJi5TW3uxpRJYdDwg2_speech.mp4\n",
      "📝 Transcription:  Hi, so I'm unable to read. I have been watching a TV series, an old one called Suits. I'm watching it because Megan Duchess of Sussex, Megan Margle stars in it. It's a story about a law firm, a very high profile law firm in New York City and Manhattan, and a young man who is brilliant, but is not a lawyer, never went to law school, but he did take the bar for somebody and passed it. And he is pretending to be a lawyer and several of his bosses know this. So he's always trying to do things to stay out of the spotlight. This law firm only hires people who went to Harvard. So he got a diploma from Harvard. Someone printed it up for him. Someone printed up grades for him. I find the whole thing to be a little bit unbelievable and just moderately entertaining.\n",
      "\n",
      "🎬 Found match for row 13: 2024-03-14T11%3A59%3A30.541Z_8Jtw40uCppYPEM2hYYlbJyPdaMl1_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-03-14T11%3A59%3A30.541Z_8Jtw40uCppYPEM2hYYlbJyPdaMl1_speech.mp4\n",
      "📝 Transcription:  Recently I am reading the book by author Ken Foley. It's his historical novel about generational family in different countries, Germany, British, and Russia, starting before World War I and continues through 1970s or 80s, so the whole century of drama and World War I and World War II. Very interesting novel. Also, what I'm doing, I'm watching tennis tournament. I'm big fan of tennis. Right now tournament goes in Indian Wells, which is in California. and I really enjoyed that much every day.\n",
      "\n",
      "🎬 Found match for row 14: 2024-05-17T12%3A22%3A40.896Z_fe3L48CUFkWUuWiDuj7auCs87qU2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-05-17T12%3A22%3A40.896Z_fe3L48CUFkWUuWiDuj7auCs87qU2_speech.mp4\n",
      "📝 Transcription:  I recently read a book called Sky Dog about the story of Dwayne Allman, the guitar player for the Allman Brothers Band. It started off in Florida where he and his brother grew up and they met other musicians and they became part members of a band. And they did some recording and then they went to Los Angeles and did more recording. And then Dwayne Allman got some studio work in Muscle Shoals, Alabama. and then Greg Allman was working in Los Angeles and then they came back to Florida and they put the band together and they got some performances and they became really well known as a jam band. And then Dwayne Allman died after recording the Live at Fillmore East album and then the album became number one seller and the band continued on after his death and became much more popular and went through a number of series of personnel changes.\n",
      "\n",
      "🎬 Found match for row 15: 2024-01-11T17%3A53%3A29.569Z_2DdhK5YnCUeu24nZuMYufCgm4F33_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-01-11T17%3A53%3A29.569Z_2DdhK5YnCUeu24nZuMYufCgm4F33_speech.mp4\n",
      "📝 Transcription:  One of the TV shows I've been watching is The Flash. I started that because I watched Arrow first, and that's also in the DC multiverse. The Flash is a really, really interesting show. I also rewatched Wednesday. I really liked that one because I watched the original Addams Family. So it was nice to see a kind of a different twist on that storyline, focusing on Wednesday as the character. Great.\n",
      "\n",
      "🎬 Found match for row 16: 2024-01-02T18%3A10%3A54.695Z_p1hkv41HcsMl4ULWqy7oJa6diG03_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-01-02T18%3A10%3A54.695Z_p1hkv41HcsMl4ULWqy7oJa6diG03_speech.mp4\n",
      "📝 Transcription:  This may sound kind of dumb, but it's January 2nd, and we watched a lot of football, including the Cleveland Browns winning again. and we watched the Rose Bowl game. And I thought it was pretty interesting the way they described the coaches and the way they also described strategies and different strengths of some of their key players. It was entertaining. And more importantly, when I watched some of these things, I learned something. that I would never have suspected. And even though I did play football, not at that level. So I guess, you know, bottom line here is that, yeah, we watched a lot of football.\n",
      "\n",
      "🎬 Found match for row 17: 2023-11-14T23%3A40%3A48.742Z_ZdL4aKmd5uSJLFc5S0ndhX4EFc92_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-11-14T23%3A40%3A48.742Z_ZdL4aKmd5uSJLFc5S0ndhX4EFc92_speech.mp4\n",
      "📝 Transcription:  I enjoyed watching today's program about the people gathering at the National Mall to support Israel. It was very informative, and I learned a lot of things I didn't know about. Another one. the news media was very cooperative with their reporting on this event. Well, I see it. I still have time to go. I'm having a hard time putting my thoughts together. Thank you.\n",
      "\n",
      "🎬 Found match for row 18: 2024-04-04T15%3A07%3A09.951Z_x1ZRr7UucUUP87nREjTLpkqAurI2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-04-04T15%3A07%3A09.951Z_x1ZRr7UucUUP87nREjTLpkqAurI2_speech.mp4\n",
      "📝 Transcription:  I just watched a documentary called Dissent, the case against Boeing, talking about the problems that Boeing has had. Essentially, Boeing was an engineering firm that started years ago and developed the first commercial jet, 707. Then they kept getting bigger and bigger and bigger. Finally, in 1997, they acquired McDonnell Douglas, which was another struggling air manufacturer. After that point, over a period of time, it went from an engineering firm to a marketing sales firm where the bottom line of shareholder value is considered more important than safety. As a consequence, they started cutting costs, outsourcing things, and taking shortcuts and kept expanding the 737 line until they got the 737 MAX, which is the one that crashed on Lion Air and Ethiopian Air. kill 376 passengers because of the decline in the engineering.\n",
      "\n",
      "🎬 Found match for row 19: 2023-12-21T13%3A48%3A50.162Z_37FzjggVoEdsAubqG770PXzdJAw2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-12-21T13%3A48%3A50.162Z_37FzjggVoEdsAubqG770PXzdJAw2_speech.mp4\n",
      "📝 Transcription:  I recently watched a movie on Netflix with Julia Roberts. I believe it's called Leave the World Behind. It has Ethan Hawke in it also. It is about, it's kind of a crazy plot of a family that goes on vacation. They're renting an Airbnb home and the owners of the home come back suddenly And they're finding out that the world around them is literally under attack. And they're trying to figure out what to do. And it's a lot about human behavior and how people act in situations where they have to save themselves or save somebody else. has lots of drama in it, a lot of plots in it that I'm not quite sure that I still understand. But it gives you an eerie feeling at the end of what would actually happen.\n",
      "\n",
      "🎬 Found match for row 20: 2024-07-25T16%3A46%3A54.847Z_J7uaam8q33bOntdwOdl5KGsuDGr2_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-07-25T16%3A46%3A54.847Z_J7uaam8q33bOntdwOdl5KGsuDGr2_speech.mp4\n",
      "📝 Transcription:  I'm in the process of reading a book by the name of How to Save Babylon. The author is Sylvia Sinclair. It's a memoir about her life as she grew up in Jamaica. And her family, her dad, really, well, her mommy and dad were part of the Rastafarian movement, which I knew nothing about before I started reading this book. I just thought it was, you know, a bunch of hippies and Bob Marley and reggae music. But it's really a whole culture with a patriarchal head of the family, and women are expected to obey and serve the men. So it's a pretty interesting read. Like I said, I didn't know anything about it before I started this book.\n",
      "\n",
      "🎬 Found match for row 21: 2024-08-28T20%3A29%3A19.186Z_8WnAOoTIZaMx8DanoDWhcOcVbpj1_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-08-28T20%3A29%3A19.186Z_8WnAOoTIZaMx8DanoDWhcOcVbpj1_speech.mp4\n",
      "📝 Transcription:  I just finished a book called Tom Lake. The main character is telling the story of her life. She started out as an actress as a young teenager, and she went to a repertory theater in Michigan in a fictitious town called Tom Lake, where she performed Our Town as Emily. And she is retelling the story of that time in her life to her three daughters during the pandemic while they were picking cherries on their cherry farm in Traverse City, Michigan. It so happened that I was driving to Traverse City, Michigan while I was listening to this book on tape. So it was very interesting. The three daughters were enthralled in her story. The mother who's telling the story is the narrator, and she does leave out some parts to her daughter that she tells us, the readers, detailing that time in her life.\n",
      "\n",
      "🎬 Found match for row 22: 2024-04-02T18%3A13%3A46.673Z_6D1DV3svDYU7WiIP2dF7SMYvS533_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-04-02T18%3A13%3A46.673Z_6D1DV3svDYU7WiIP2dF7SMYvS533_speech.mp4\n",
      "📝 Transcription:  I'm currently reading a book called Detective in the Courtyard. It's an interesting book. It's about the reflections of a Maine police officer. This man had come up from someplace in New York, New Jersey area and retired in Maine as a policeman. He describes the things that are different about the environment in Maine, where it's a big city, an urban sprawl, if you will, than it is in the East Coast. It's really kind of neat. It talks about a lot of things that people don't realize that police officers have to deal with every day. And a lot of them are funny, and some of them are not so funny. it's an interesting book and I think that if you have a chance to read it Timothy Cotter is the author and it's neat take care\n",
      "\n",
      "🎬 Found match for row 23: 2024-04-12T00%3A04%3A53.506Z_pYVvhH67OANo9hxBSp42b6yzwXA3_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-04-12T00%3A04%3A53.506Z_pYVvhH67OANo9hxBSp42b6yzwXA3_speech.mp4\n",
      "📝 Transcription:  Yes, the TV show I've watched that actually got my attention was Game of Thrones. Yeah, it was an interesting, it was quite interesting. picturing the dragons and how they conquered kingdoms and different family fighting over each other. I really enjoyed the TV show. It also involved being who had the most power ruled. It was quite interesting. Another TV show I enjoyed was The Legend of the Seeker. An old movie though but it actually involves magic and sword. The Seeker have to bring peace to the world and conquer evil. That one was actually interesting too. I enjoyed it. So those are the two TV shows I tried it in here. It was worth watching and I loved it.\n",
      "\n",
      "🎬 Found match for row 24: 2024-04-12T17%3A44%3A49.579Z_oNNSVaS5pXS4m2J6DI12g5BtDl22_speech.mp4\n",
      "Full path: /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2024-04-12T17%3A44%3A49.579Z_oNNSVaS5pXS4m2J6DI12g5BtDl22_speech.mp4\n",
      "📝 Transcription:  I recently watched a Perry Mason episode of a series from, I don't know when, quite a long time ago. It was in black and white. It was about a woman who was trying to murder her boss, I think he was. and she in order to take the blame off her take any suspicion off her she sent herself some expensive chocolates that were laced with poison as it turned out of course Perry Mason who doesn't lose any case was able to figure out what happened and got her to admit to it.\n",
      "\n",
      "🎬 Processing video: 2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: facial_expressions_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: BG_and_lighting_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Blink_rate_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Camera_position_query\n",
      "🔄 Modified prompt Coherence_query with transcription\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Coherence_query\n",
      "🔄 Modified prompt Comp_task_query with transcription\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Comp_task_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Lip_parting_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Masked_faceExp_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Observations_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Overall_app_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Other_people_present_query\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed prompt: Other_BP_visible_query\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "\n",
    "def transcribe_videos(input_df, video_path, pipe_asr):\n",
    "    input_df = input_df.copy()\n",
    "    if \"Transcription\" not in input_df.columns:\n",
    "        input_df[\"Transcription\"] = None\n",
    "    \n",
    "    available_videos = [f for f in os.listdir(video_path) if f.endswith(\".mp4\")]\n",
    "    print(f\"Found {len(available_videos)} videos in directory\")\n",
    "    \n",
    "    for idx, row in input_df.iterrows():\n",
    "        video_name = row.get(\"video\", None)\n",
    "        if not isinstance(video_name, str) or not video_name:\n",
    "            continue\n",
    "        \n",
    "        # Find exact matching file\n",
    "        matched_file = None\n",
    "        for filename in available_videos:\n",
    "            if video_name[-25:] in filename:\n",
    "                matched_file = filename\n",
    "                break\n",
    "        \n",
    "        if matched_file:\n",
    "            full_video_path = os.path.join(video_path, matched_file)\n",
    "            print(f\"\\n🎬 Found match for row {idx}: {matched_file}\")\n",
    "            print(f\"Full path: {full_video_path}\")\n",
    "            \n",
    "            try:\n",
    "                # Use a temporary file with unique name\n",
    "                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:\n",
    "                    tmp_audio_file = temp_audio.name\n",
    "                \n",
    "                # Convert video to audio\n",
    "                audio = AudioSegment.from_file(full_video_path, format=\"mp4\")\n",
    "                audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "                audio.export(tmp_audio_file, format=\"wav\")\n",
    "                \n",
    "                # Perform transcription\n",
    "                result = pipe_asr(tmp_audio_file)\n",
    "                transcription_text = result[\"text\"]\n",
    "                \n",
    "                # Add to dataframe\n",
    "                input_df.loc[idx, \"Transcription\"] = transcription_text\n",
    "                print(f\"📝 Transcription: {transcription_text}\")\n",
    "                \n",
    "                # Clean up temp file\n",
    "                os.remove(tmp_audio_file)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to process video {matched_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"❌ No matching video found for row {idx}, video name: {video_name}\")\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def process_all_prompts(input_df, prompt_dict, video_path, analyze_video, nami_df=None, natalia_df=None):\n",
    "    available_videos = [f for f in os.listdir(video_path) if f.endswith(\".mp4\")]\n",
    "    \n",
    "    # List of prompts that need transcription\n",
    "    transcription_prompts = [\"Coherence_query\", \"Comp_task_query\", \"Comp_sent_query\"]\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    columns = [\"Video_file\", \"Q_ID\", \"Prompt_name\", \"Model_Name\", \"Output\", \"Cayla_Resp\", \"Nami_Resp\", \"Natalia_Resp\"]\n",
    "    output = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # Process each row in the input DataFrame\n",
    "    for idx, row in input_df.iterrows():\n",
    "        video_name = row.get(\"video\", None)\n",
    "        if not isinstance(video_name, str) or not video_name:\n",
    "            continue\n",
    "            \n",
    "        # Get the transcription\n",
    "        transcription = row.get(\"Transcription\", \"\")\n",
    "        \n",
    "        # Find matching video file\n",
    "        matched_file = None\n",
    "        for f in available_videos:\n",
    "            if video_name[-25:] in f:\n",
    "                matched_file = f\n",
    "                break\n",
    "    \n",
    "        if matched_file:\n",
    "            full_video_path = os.path.join(video_path, matched_file)\n",
    "            print(f\"\\n🎬 Processing video: {matched_file}\")\n",
    "            \n",
    "            # Get prompt columns for annotations\n",
    "            prompt_columns = list(input_df.columns[0:13])  # First 13 columns as per original code\n",
    "            \n",
    "            # Process each prompt in the dictionary\n",
    "            for q_id, (prompt_name, prompt_text) in enumerate(prompt_dict.items(), start=1):\n",
    "                # For the 3 specific prompts, insert transcription if available\n",
    "                if prompt_name in transcription_prompts and isinstance(transcription, str) and transcription:\n",
    "                    prompt_text = prompt_text.replace(\"<TRANSCRIPTION_OUTPUT>\", transcription)\n",
    "                    print(f\"🔄 Modified prompt {prompt_name} with transcription\")\n",
    "                \n",
    "                # Get responses for comparison\n",
    "                cayla_resp = row[prompt_columns[q_id - 1]] if q_id - 1 < len(prompt_columns) else \"\"\n",
    "                \n",
    "                nami_resp = \"\"\n",
    "                if nami_df is not None:\n",
    "                    nami_row = nami_df[nami_df[\"video\"] == video_name]\n",
    "                    if not nami_row.empty and q_id - 1 < len(prompt_columns):\n",
    "                        nami_resp = nami_row[prompt_columns[q_id - 1]].values[0]\n",
    "                \n",
    "                natalia_resp = \"\"\n",
    "                if natalia_df is not None:\n",
    "                    natalia_row = natalia_df[natalia_df[\"video\"] == video_name]\n",
    "                    if not natalia_row.empty and q_id - 1 < len(prompt_columns):\n",
    "                        natalia_resp = natalia_row[prompt_columns[q_id - 1]].values[0]\n",
    "                \n",
    "                # Process with Kangaroo model\n",
    "                response = analyze_video(full_video_path, prompt_text)\n",
    "                print(f\"✅ Completed prompt: {prompt_name}\")\n",
    "                \n",
    "                # Add to results DataFrame\n",
    "                new_row = {\n",
    "                    \"Video_file\": matched_file,\n",
    "                    \"Q_ID\": q_id,\n",
    "                    \"Prompt_name\": prompt_name,\n",
    "                    \"Model_Name\": \"Kangaroo\",\n",
    "                    \"Output\": response,\n",
    "                    \"Cayla_Resp\": cayla_resp,\n",
    "                    \"Nami_Resp\": nami_resp,\n",
    "                    \"Natalia_Resp\": natalia_resp\n",
    "                }\n",
    "                \n",
    "                output = pd.concat([output, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"❌ No matching video found for: {video_name}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# First transcribe the videos\n",
    "cayla_df_with_transcriptions = transcribe_videos(\n",
    "    input_df=cayla_df,\n",
    "    video_path=video_path,\n",
    "    pipe_asr=pipe_asr\n",
    ")\n",
    "\n",
    "\n",
    "# Then process all prompts (inserting transcriptions only for the 3 specified prompts)\n",
    "results_df = process_all_prompts(\n",
    "    input_df=cayla_df_with_transcriptions,\n",
    "    prompt_dict=prompt_dict,\n",
    "    video_path=video_path,\n",
    "    analyze_video=analyze_video,\n",
    "    nami_df=nami_df,\n",
    "    natalia_df=natalia_df\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"kangaroo_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localdisk4/panwla/Projects/park_vlm/Kangaroo'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cayla_path = '/localdisk4/panwla/Projects/park_vlm/Annotations/Cayla'\n",
    "os.chdir(cayla_path)\n",
    "\n",
    "cayla_df_with_transcriptions.to_csv(\"cayla_df_with_transcriptions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ratan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 full_video_path : /localdisk4/panwla/Projects/park_vlm/Videos/Videos/2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4\n",
      "\n",
      "🎬 Processing video: 2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4\n",
      "✅ Matched Nami row index: 2, and video: 2    2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...\n",
      "Name: video, dtype: object\n",
      "✅ Matched Natalia row index: 0, and video: 0    2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...\n",
      "Name: video, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === Empty DataFrame to collect results ===\n",
    "columns = [\"Video_file\", \"Q_ID\", \"Prompt_name\", \"Model_Name\", \"Cayla_Resp\", \"Nami_Resp\", \"Natalia_Resp\"]\n",
    "output = pd.DataFrame(columns=columns)\n",
    "\n",
    "for row_idx, record in enumerate(cayla_df[\"video\"][0:1]):  # loop over first row\n",
    "    if not isinstance(record, str):\n",
    "        continue\n",
    "    # Find matching video file from available .mp4 files\n",
    "    matched_file = None\n",
    "    for f in available_videos:\n",
    "        if record[-25:] in f:\n",
    "            matched_file = f\n",
    "            break\n",
    "\n",
    "    if matched_file:\n",
    "        full_video_path = os.path.join(video_path, matched_file)\n",
    "        print(f\"\\n🎬 full_video_path : {full_video_path}\")\n",
    "        print(f\"\\n🎬 Processing video: {matched_file}\")\n",
    "\n",
    "        # === Find corresponding rows in other annotator DataFrames based on 'video' column ===\n",
    "        cayla_row = cayla_df[cayla_df[\"video\"] == record]\n",
    "\n",
    "        nami_row = nami_df[nami_df[\"video\"] == record]\n",
    "        natalia_row = natalia_df[natalia_df[\"video\"] == record]\n",
    "\n",
    "        # Print matched row indices (if found)\n",
    "        if not nami_row.empty:\n",
    "            print(f\"✅ Matched Nami row index: {nami_row.index[0]}, and video: {nami_row['video']}\")\n",
    "        else:\n",
    "            print(\"❌ No match found in nami_df\")\n",
    "\n",
    "        if not natalia_row.empty:\n",
    "            print(f\"✅ Matched Natalia row index: {natalia_row.index[0]}, and video: {natalia_row['video']}\")\n",
    "        else:\n",
    "            print(\"❌ No match found in natalia_df\")\n",
    "\n",
    "        prompt_columns = list(cayla_df.columns[0:13])  # convert to list\n",
    "\n",
    "        for idx, (prompt_name, prompt_text) in enumerate(prompt_dict.items(), start=1):\n",
    "            cayla_resp = cayla_row[prompt_columns[idx - 1]].values[0] if not cayla_row.empty else \"\"\n",
    "            nami_resp = nami_row[prompt_columns[idx - 1]].values[0] if not nami_row.empty else \"\"\n",
    "            natalia_resp = natalia_row[prompt_columns[idx - 1]].values[0] if not natalia_row.empty else \"\"\n",
    "\n",
    "            response = analyze_video(full_video_path, prompt_text)\n",
    "\n",
    "            new_row = {\n",
    "                \"Video_file\": matched_file,\n",
    "                \"Q_ID\": idx,\n",
    "                \"Prompt_name\": prompt_name,\n",
    "                \"Model_Name\": \"Kangaroo\",\n",
    "                \"Output\": response,\n",
    "                \"Cayla_Resp\": cayla_resp,\n",
    "                \"Nami_Resp\": nami_resp,\n",
    "                \"Natalia_Resp\": natalia_resp\n",
    "            }\n",
    "\n",
    "            output = pd.concat([output, pd.DataFrame([new_row])], ignore_index=True) # type: ignore\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ No matching video found for: {record}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3mAT7hTdwpOV2_resting_face.mp4'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output['Video_file'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_file</th>\n",
       "      <th>Q_ID</th>\n",
       "      <th>Prompt_name</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Cayla_Resp</th>\n",
       "      <th>Nami_Resp</th>\n",
       "      <th>Natalia_Resp</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>1</td>\n",
       "      <td>facial_expressions_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>none</td>\n",
       "      <td>perhaps slight, rubs head early in clip when t...</td>\n",
       "      <td>no</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>2</td>\n",
       "      <td>BG_and_lighting_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>normal background</td>\n",
       "      <td>lighting is a little low</td>\n",
       "      <td>normal background</td>\n",
       "      <td>Normal background.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>3</td>\n",
       "      <td>Blink_rate_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>reduced</td>\n",
       "      <td>decreased</td>\n",
       "      <td>Blink rate was reduced.</td>\n",
       "      <td>Normal blinking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>4</td>\n",
       "      <td>Camera_position_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>too close</td>\n",
       "      <td>good</td>\n",
       "      <td>good position</td>\n",
       "      <td>A good position of the camera would be when th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>5</td>\n",
       "      <td>Coherence_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>coherent</td>\n",
       "      <td>coherent</td>\n",
       "      <td>normal coherence, easy to understand story</td>\n",
       "      <td>The subject is coherent in what they are speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>6</td>\n",
       "      <td>Comp_task_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>Yes, follows instructions</td>\n",
       "      <td>compliant</td>\n",
       "      <td>Yes, she completes the task appropriately.</td>\n",
       "      <td>The subject followed the instructions to talk ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>7</td>\n",
       "      <td>Lip_parting_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>very few times</td>\n",
       "      <td>Unable to grade, never really at rest</td>\n",
       "      <td>very few times (only able to assess for a shor...</td>\n",
       "      <td>Most of the time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>8</td>\n",
       "      <td>Masked_faceExp_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>neither</td>\n",
       "      <td>The individual’s face appears blank and emotio...</td>\n",
       "      <td>The individual’s face appears emotionless</td>\n",
       "      <td>(I) The individual's face appears blank and em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>9</td>\n",
       "      <td>Observations_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>cannot view other body parts</td>\n",
       "      <td>slight possible dyskinetic movements of head a...</td>\n",
       "      <td>none noted</td>\n",
       "      <td>The video shows a woman in a room with a chest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>10</td>\n",
       "      <td>Overall_app_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>anxious</td>\n",
       "      <td>looks slightly tired</td>\n",
       "      <td>calm</td>\n",
       "      <td>The subject is appearing to be tired or exhaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>11</td>\n",
       "      <td>Other_people_present_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>no</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...</td>\n",
       "      <td>12</td>\n",
       "      <td>Other_BP_visible_query</td>\n",
       "      <td>Kangaroo</td>\n",
       "      <td>simple sentences</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>No.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Video_file Q_ID  \\\n",
       "0   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    1   \n",
       "1   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    2   \n",
       "2   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    3   \n",
       "3   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    4   \n",
       "4   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    5   \n",
       "5   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    6   \n",
       "6   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    7   \n",
       "7   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    8   \n",
       "8   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...    9   \n",
       "9   2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...   10   \n",
       "10  2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...   11   \n",
       "11  2023-05-11T21%3A59%3A07.908Z_vBAkVvJKtggUnpO3m...   12   \n",
       "\n",
       "                   Prompt_name Model_Name                    Cayla_Resp  \\\n",
       "0     facial_expressions_query   Kangaroo                          none   \n",
       "1        BG_and_lighting_query   Kangaroo             normal background   \n",
       "2             Blink_rate_query   Kangaroo                       reduced   \n",
       "3        Camera_position_query   Kangaroo                     too close   \n",
       "4              Coherence_query   Kangaroo                      coherent   \n",
       "5              Comp_task_query   Kangaroo     Yes, follows instructions   \n",
       "6            Lip_parting_query   Kangaroo                very few times   \n",
       "7         Masked_faceExp_query   Kangaroo                       neither   \n",
       "8           Observations_query   Kangaroo  cannot view other body parts   \n",
       "9            Overall_app_query   Kangaroo                       anxious   \n",
       "10  Other_people_present_query   Kangaroo                          none   \n",
       "11      Other_BP_visible_query   Kangaroo              simple sentences   \n",
       "\n",
       "                                            Nami_Resp  \\\n",
       "0   perhaps slight, rubs head early in clip when t...   \n",
       "1                           lighting is a little low    \n",
       "2                                          decreased    \n",
       "3                                                good   \n",
       "4                                            coherent   \n",
       "5                                           compliant   \n",
       "6              Unable to grade, never really at rest    \n",
       "7   The individual’s face appears blank and emotio...   \n",
       "8   slight possible dyskinetic movements of head a...   \n",
       "9                               looks slightly tired    \n",
       "10                                              none    \n",
       "11                                                no    \n",
       "\n",
       "                                         Natalia_Resp  \\\n",
       "0                                                  no   \n",
       "1                                   normal background   \n",
       "2                            Blink rate was reduced.    \n",
       "3                                       good position   \n",
       "4          normal coherence, easy to understand story   \n",
       "5         Yes, she completes the task appropriately.    \n",
       "6   very few times (only able to assess for a shor...   \n",
       "7           The individual’s face appears emotionless   \n",
       "8                                          none noted   \n",
       "9                                                calm   \n",
       "10                                                 no   \n",
       "11                                                 no   \n",
       "\n",
       "                                               Output  \n",
       "0                                                 No.  \n",
       "1                                  Normal background.  \n",
       "2                                    Normal blinking.  \n",
       "3   A good position of the camera would be when th...  \n",
       "4   The subject is coherent in what they are speak...  \n",
       "5   The subject followed the instructions to talk ...  \n",
       "6                                   Most of the time.  \n",
       "7   (I) The individual's face appears blank and em...  \n",
       "8   The video shows a woman in a room with a chest...  \n",
       "9   The subject is appearing to be tired or exhaus...  \n",
       "10                                                No.  \n",
       "11                                                No.  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output.to_csv(\"cayla_model_outputs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 96.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:\n",
      " The latest series I just watched was Firefly and Lane. It's about two girls growing up as friends. They've had their share of arguments, fights, whatever. And shows them all through their whole life.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load the pre-trained Whisper model\n",
    "model = whisper.load_model(\"base\")  # options: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "\n",
    "# Transcribe an audio file\n",
    "result = model.transcribe(full_video_path)  # or .wav, .m4a, etc.\n",
    "\n",
    "# Print the transcript\n",
    "print(\"Transcription:\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0077aff3cd244cb8b165a3b581f5fce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d689ac1981d24f24a5d08bcaa6c301d3",
      "placeholder": "​",
      "style": "IPY_MODEL_d7ea7f3c9c3941369ac9bdd33c641675",
      "value": "data_utils.py: 100%"
     }
    },
    "0395e534500242e6b3b6cc5166c7de46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "098d6452e23344dd9407d96b4aa97ce6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09ac990f40c843be82d7809c7437b054": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e2ceed1e07441b9b67e760961320733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43f477f864374a9d951d2c76833c1757",
       "IPY_MODEL_3444bd524cb54bfeb29dbd8837016dcd",
       "IPY_MODEL_4531e3b66fd646799fac4d68386d6402"
      ],
      "layout": "IPY_MODEL_5b3d67d4e65d4877b9f056a81fbc7c6f"
     }
    },
    "118890970cdd40fdbd36261434afa82c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1382d1d09fba42118e4c2e496c0c99c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163046e0070b4c099f7dbcd8a9e81b6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "174cd456b61f4420aaf403f78ac5da71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9df03c476e394263816adb2f2818e2f1",
      "placeholder": "​",
      "style": "IPY_MODEL_4caf53732a5d45ed903d0427ef1a0252",
      "value": "config.json: 100%"
     }
    },
    "17f3fc0c5b094de8bf9d23989eafe897": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f859d72580c64b9eb7ab0a43693939d3",
      "max": 73,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8610cb35c2ac4cb68e51f4d05add66f7",
      "value": 73
     }
    },
    "1cd3275bbcc74ea6bbd67bb603db8f55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f948f537ff5141cfbc28189e0f5b9702",
      "max": 50791,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83ee0e28df904e129c0a1b8a84f980ac",
      "value": 50791
     }
    },
    "1e12718580954522866aff518c8f8bb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ea428939115402f8960dbc6fad7b246": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "213150106e2d4e658f84200c9695d11b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2531e981c9994fb8b86d0c19c7ddc028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3321030fe04e4a2785662e6d4ee01c59",
       "IPY_MODEL_2a0bdce803ae40b28abf422b0cf5f9e0",
       "IPY_MODEL_3e26288235a5451faab43b88640c70b6"
      ],
      "layout": "IPY_MODEL_1ea428939115402f8960dbc6fad7b246"
     }
    },
    "27be7925bd584307ac38bbe0ab224f36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a0bdce803ae40b28abf422b0cf5f9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e37fa30d97684652b6e6d78b167496ec",
      "max": 30144,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0d47bb4b5054a71bba33778880888ad",
      "value": 30144
     }
    },
    "2de7bc16b6794db48655c1b4e8cf197b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "328ff4177c9944468d666e3d90580a7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3321030fe04e4a2785662e6d4ee01c59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0395e534500242e6b3b6cc5166c7de46",
      "placeholder": "​",
      "style": "IPY_MODEL_828741f9bc084888815ab3104ca35dc8",
      "value": "vision_tower_builder.py: 100%"
     }
    },
    "3444bd524cb54bfeb29dbd8837016dcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_213150106e2d4e658f84200c9695d11b",
      "max": 16717357520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09ac990f40c843be82d7809c7437b054",
      "value": 16717357520
     }
    },
    "37c6345e889e4709ba7929f86ed90c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b0beb9804dd4c3f9083448a84bb3be6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_595e2dd800574bb884bbe2b614ef3da4",
       "IPY_MODEL_d65c541feeac4ffbb741032c6a177b56",
       "IPY_MODEL_8f48e66f7a2d467c87427076dee413d2"
      ],
      "layout": "IPY_MODEL_65560e1b01da4067a844c89d40078fb0"
     }
    },
    "3b411a58f36e4e8aa66a393f6c20e207": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ce82b0f3aa6428598f510618cad16b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e0ef9781a294fc48c44513b53ff5a5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e26288235a5451faab43b88640c70b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7175f33be6b44134a26c7ecb979a9d52",
      "placeholder": "​",
      "style": "IPY_MODEL_7847aa11333e454aa8560d1f36cd37df",
      "value": " 30.1k/30.1k [00:00&lt;00:00, 975kB/s]"
     }
    },
    "3e375821ab46401399a5b8a57b1c5d9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f872e8c5d66417b8686cb873eeff370": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42562de8470e47feb0774749c8139700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2de7bc16b6794db48655c1b4e8cf197b",
      "max": 121,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d73bed56434743f9b1271e3e77a46b81",
      "value": 121
     }
    },
    "43f477f864374a9d951d2c76833c1757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d21ff2ef4f4947d0b2f019c0bf129f8f",
      "placeholder": "​",
      "style": "IPY_MODEL_e59611d0b20b4142a94bbeaf0ef74950",
      "value": "model.safetensors: 100%"
     }
    },
    "44e96ec662f6499ab082732268905afd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6297f0f4bc2a4a3eb89506553581f86d",
      "placeholder": "​",
      "style": "IPY_MODEL_e85a57d3a8f74deb845f86789e52cfe6",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4531e3b66fd646799fac4d68386d6402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e12718580954522866aff518c8f8bb6",
      "placeholder": "​",
      "style": "IPY_MODEL_7893aced91e44f2d9b1f6602b42388fd",
      "value": " 16.7G/16.7G [06:38&lt;00:00, 42.4MB/s]"
     }
    },
    "45f82d3f47c14b5e8870032d5613510c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "469c711c363e4ff7a7a6aba724fbf7a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "491520d3c65b4ca68bc1f19764de60c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0077aff3cd244cb8b165a3b581f5fce7",
       "IPY_MODEL_df3c837b838a4dc79a0475636f14c6d8",
       "IPY_MODEL_c15679adda4d43c0a3ae4e21c74d83ca"
      ],
      "layout": "IPY_MODEL_27be7925bd584307ac38bbe0ab224f36"
     }
    },
    "4caf53732a5d45ed903d0427ef1a0252": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50e3ff97ca2b44fd8410c0d778e8a558": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "536ef1cfc9f7419caa126be7376bce25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce9d0d79de4d4d4ebedf4d8005836e31",
      "placeholder": "​",
      "style": "IPY_MODEL_5f231b0182bb472abe40b6a92361bba5",
      "value": " 9.08M/9.08M [00:00&lt;00:00, 27.0MB/s]"
     }
    },
    "53e72ad518a149b798385ca63e398e59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93683972f38a44aa9df4efa655251a18",
       "IPY_MODEL_17f3fc0c5b094de8bf9d23989eafe897",
       "IPY_MODEL_6f9a1df832fd408db50a56e2b033d7f2"
      ],
      "layout": "IPY_MODEL_1382d1d09fba42118e4c2e496c0c99c3"
     }
    },
    "5523f023b99549f8bdc6e2d384ba5bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5749c7642a8042a5b19e1dd7f13c0460": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "595e2dd800574bb884bbe2b614ef3da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a70884ad1bf844138389f86bc37fc5a5",
      "placeholder": "​",
      "style": "IPY_MODEL_469c711c363e4ff7a7a6aba724fbf7a4",
      "value": "mm_projector_builder.py: 100%"
     }
    },
    "5b3d67d4e65d4877b9f056a81fbc7c6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f07306ad9d645d18ca96cb7c9370039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50e3ff97ca2b44fd8410c0d778e8a558",
      "placeholder": "​",
      "style": "IPY_MODEL_d8d319c19899422f8eb3fc8007e2aef2",
      "value": " 50.8k/50.8k [00:00&lt;00:00, 2.17MB/s]"
     }
    },
    "5f231b0182bb472abe40b6a92361bba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6151de5c5880437b8289ba5aed06e45b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61b8dc6397114c8187ae49b444b0df57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a40bd4b8259d42f694477900bcbdb640",
      "placeholder": "​",
      "style": "IPY_MODEL_bf5feb1e1eb94bb59fd26af7637372ac",
      "value": "generation_config.json: 100%"
     }
    },
    "6297f0f4bc2a4a3eb89506553581f86d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65560e1b01da4067a844c89d40078fb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cb548e20cd649e48e61942e01e86e9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f9a1df832fd408db50a56e2b033d7f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f872e8c5d66417b8686cb873eeff370",
      "placeholder": "​",
      "style": "IPY_MODEL_a86295bf80744b35af52de413cd06297",
      "value": " 73.0/73.0 [00:00&lt;00:00, 2.45kB/s]"
     }
    },
    "711ff6e69eab482aafee16d55aa6a580": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7175f33be6b44134a26c7ecb979a9d52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74617de217bb478aa3665791e6ac172f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74e1edc1fa694df4a541b9a90a885e59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f2b53a8be384c0bb460af33ba7e3a40",
      "placeholder": "​",
      "style": "IPY_MODEL_5749c7642a8042a5b19e1dd7f13c0460",
      "value": "tokenizer.json: 100%"
     }
    },
    "7519b9e359924715b8f924048de684e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7847aa11333e454aa8560d1f36cd37df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7893aced91e44f2d9b1f6602b42388fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79ae1738893e41cdaa9298e483c7e34b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_74e1edc1fa694df4a541b9a90a885e59",
       "IPY_MODEL_cf183ac5fa2240ff8e96c03a006ccd6a",
       "IPY_MODEL_536ef1cfc9f7419caa126be7376bce25"
      ],
      "layout": "IPY_MODEL_8634809d5d5242cf934e2e5a806ca635"
     }
    },
    "7c86c52e743e4c819bd55ecad8cd62d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d8dabcb35634a4987e0c9a7eddad2d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_118890970cdd40fdbd36261434afa82c",
      "placeholder": "​",
      "style": "IPY_MODEL_7c86c52e743e4c819bd55ecad8cd62d2",
      "value": " 121/121 [00:00&lt;00:00, 7.43kB/s]"
     }
    },
    "828741f9bc084888815ab3104ca35dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "835d2746ae9144dda4ba25a3ae9f9ff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb52220ed16448efaaeb0d276e111363",
      "max": 67860,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b411a58f36e4e8aa66a393f6c20e207",
      "value": 67860
     }
    },
    "83ee0e28df904e129c0a1b8a84f980ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8610cb35c2ac4cb68e51f4d05add66f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8629ee0f787f44cfa93c07613cf11b38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6cb548e20cd649e48e61942e01e86e9c",
      "placeholder": "​",
      "style": "IPY_MODEL_163046e0070b4c099f7dbcd8a9e81b6f",
      "value": " 822/822 [00:00&lt;00:00, 43.6kB/s]"
     }
    },
    "8634809d5d5242cf934e2e5a806ca635": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ab929cc81d040329de8d43afe892603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9738241214c4400091c40d4b941d3c6b",
       "IPY_MODEL_835d2746ae9144dda4ba25a3ae9f9ff0",
       "IPY_MODEL_af14a0e445284a018eb8ce42f95a6c05"
      ],
      "layout": "IPY_MODEL_3ce82b0f3aa6428598f510618cad16b7"
     }
    },
    "8f2b53a8be384c0bb460af33ba7e3a40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f48e66f7a2d467c87427076dee413d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5523f023b99549f8bdc6e2d384ba5bf1",
      "placeholder": "​",
      "style": "IPY_MODEL_37c6345e889e4709ba7929f86ed90c69",
      "value": " 1.86k/1.86k [00:00&lt;00:00, 91.3kB/s]"
     }
    },
    "93683972f38a44aa9df4efa655251a18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acfd17d8cdcf4b639b2b0440691a2611",
      "placeholder": "​",
      "style": "IPY_MODEL_098d6452e23344dd9407d96b4aa97ce6",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "9738241214c4400091c40d4b941d3c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7d7acb176034a49bc2b406c134f72e5",
      "placeholder": "​",
      "style": "IPY_MODEL_328ff4177c9944468d666e3d90580a7b",
      "value": "modeling_kangaroo.py: 100%"
     }
    },
    "9df03c476e394263816adb2f2818e2f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a02fe548f1384191bce078862365d309": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0fe0a5796b44d798f644d8e24d49313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_61b8dc6397114c8187ae49b444b0df57",
       "IPY_MODEL_42562de8470e47feb0774749c8139700",
       "IPY_MODEL_7d8dabcb35634a4987e0c9a7eddad2d9"
      ],
      "layout": "IPY_MODEL_a315c4f720734edc9a12a27c6b6ace75"
     }
    },
    "a315c4f720734edc9a12a27c6b6ace75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a40bd4b8259d42f694477900bcbdb640": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a70884ad1bf844138389f86bc37fc5a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a86295bf80744b35af52de413cd06297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acfd17d8cdcf4b639b2b0440691a2611": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af14a0e445284a018eb8ce42f95a6c05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d62490cc3755445b92806236bf7816a3",
      "placeholder": "​",
      "style": "IPY_MODEL_7519b9e359924715b8f924048de684e8",
      "value": " 67.9k/67.9k [00:00&lt;00:00, 3.77MB/s]"
     }
    },
    "b4f886173b7f40a59955684abe416e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf5feb1e1eb94bb59fd26af7637372ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c15679adda4d43c0a3ae4e21c74d83ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca90535ddb32442d8759673284069133",
      "placeholder": "​",
      "style": "IPY_MODEL_b4f886173b7f40a59955684abe416e96",
      "value": " 5.43k/5.43k [00:00&lt;00:00, 184kB/s]"
     }
    },
    "c7d7acb176034a49bc2b406c134f72e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c97bd7b5740344889633352e6bc08feb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_174cd456b61f4420aaf403f78ac5da71",
       "IPY_MODEL_f3a1ea16c26143419d19cb872ebcda44",
       "IPY_MODEL_8629ee0f787f44cfa93c07613cf11b38"
      ],
      "layout": "IPY_MODEL_fc81b34e5f634ee4842c67bd97fe0ef5"
     }
    },
    "ca90535ddb32442d8759673284069133": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce9d0d79de4d4d4ebedf4d8005836e31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf183ac5fa2240ff8e96c03a006ccd6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a02fe548f1384191bce078862365d309",
      "max": 9084340,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e0ef9781a294fc48c44513b53ff5a5f",
      "value": 9084340
     }
    },
    "d0d47bb4b5054a71bba33778880888ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d21ff2ef4f4947d0b2f019c0bf129f8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3c4fe978ccf4249b1d8581c930d7432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_44e96ec662f6499ab082732268905afd",
       "IPY_MODEL_1cd3275bbcc74ea6bbd67bb603db8f55",
       "IPY_MODEL_5f07306ad9d645d18ca96cb7c9370039"
      ],
      "layout": "IPY_MODEL_6151de5c5880437b8289ba5aed06e45b"
     }
    },
    "d62490cc3755445b92806236bf7816a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d65c541feeac4ffbb741032c6a177b56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_711ff6e69eab482aafee16d55aa6a580",
      "max": 1864,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ee2981c4d18b4f72b231beb9ab422ca7",
      "value": 1864
     }
    },
    "d689ac1981d24f24a5d08bcaa6c301d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d73bed56434743f9b1271e3e77a46b81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d7ea7f3c9c3941369ac9bdd33c641675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8d319c19899422f8eb3fc8007e2aef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddae58cbb2304c18bbc97ee3a815c059": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df3c837b838a4dc79a0475636f14c6d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74617de217bb478aa3665791e6ac172f",
      "max": 5427,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45f82d3f47c14b5e8870032d5613510c",
      "value": 5427
     }
    },
    "e37fa30d97684652b6e6d78b167496ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e59611d0b20b4142a94bbeaf0ef74950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e85a57d3a8f74deb845f86789e52cfe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb52220ed16448efaaeb0d276e111363": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee2981c4d18b4f72b231beb9ab422ca7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3a1ea16c26143419d19cb872ebcda44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e375821ab46401399a5b8a57b1c5d9e",
      "max": 822,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ddae58cbb2304c18bbc97ee3a815c059",
      "value": 822
     }
    },
    "f859d72580c64b9eb7ab0a43693939d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f948f537ff5141cfbc28189e0f5b9702": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc81b34e5f634ee4842c67bd97fe0ef5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
