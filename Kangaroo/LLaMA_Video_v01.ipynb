{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/localdisk4/panwla/Projects/park_vlm/Kangaroo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.getcwd()\n",
    "os.chdir(\"/localdisk4/panwla/Projects/park_vlm/Kangaroo\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /localdisk4/panwla/huggingface_cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import yaml\n",
    "# from google.colab import drive\n",
    "\n",
    "# # Mount Google Drive to access API keys\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Load API keys from file\n",
    "file_path = './.API_KEYS/API_KEYS.yml'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    api_keys = yaml.safe_load(file)\n",
    "\n",
    "hf_read_api_key = api_keys['HUGGINGFACE']['HF_READ_API_KEY']\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(hf_read_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables based on your bashrc settings\n",
    "os.environ[\"JUPYTER_DATA_DIR\"] = \"/localdisk4/panwla/jupyter_data\"\n",
    "os.environ[\"HF_HOME\"] = \"/localdisk4/panwla/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/localdisk4/panwla/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/localdisk4/panwla/huggingface_cache/datasets\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/localdisk4/panwla/torch\"\n",
    "os.environ[\"XDG_CACHE_HOME\"] = \"/localdisk4/panwla_cache/.cache\"\n",
    "os.environ[\"TMPDIR\"] = \"/localdisk4/panwla/tmp\"\n",
    "os.environ[\"TEMP\"] = \"/localdisk4/panwla/tmp\"\n",
    "os.environ[\"TMP\"] = \"/localdisk4/panwla/tmp\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "directories = [\n",
    "    os.environ[\"JUPYTER_DATA_DIR\"],\n",
    "    os.environ[\"HF_HOME\"],\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"],\n",
    "    os.environ[\"HF_DATASETS_CACHE\"],\n",
    "    os.environ[\"TORCH_HOME\"],\n",
    "    os.environ[\"XDG_CACHE_HOME\"],\n",
    "    os.environ[\"TMPDIR\"],\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facial_expressions_query = '''\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: Please describe whether the person demonstrates any difficulty through their facial expressions. Some examples of visible difficulty include furrowed brow, squinting eyes, clenched jaw, tight lips, head hanging low, sighing, wrinkled forehead, etc. Mention such specific details when found. End output with a final answer choice: \"Yes\" or \"No\".\n",
    "\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "\n",
    "Apparent_diff_task ='''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video. \n",
    "\n",
    "Question: Please describe whether the person demonstrates any difficulty through their facial expressions. Some examples of visible difficulty include furrowed brow, squinting eyes, clenched jaw, tight lips, head hanging low, sighing, wrinkled forehead, etc. Mention such specific details when found. End output with a final answer choice: “Yes” or “No”.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "BG_and_lighting_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: Mention if the background is overloaded (i.e., too many things), or the lighting condition is inappropriate (i.e., too dark or overlit). Otherwise, just output “normal background”.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Blink_rate_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: Was there anything abnormal about the person’s eye blink rate? For example, they may not be blinking at all, or they may have reduced or increased blink rate compared to a normal person. If there is nothing abnormal, output “normal blinking”.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Camera_position_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "Question: How far is the camera? A good position of the camera would be when the upper half of the subject's body remains visible, while the lower half is not captured in the frame. If the upper body is only partly visible, the camera is too close. If the lower body is also visible, the camera is too far.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Coherence_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Analyze the provided text transcription of the person’s speech and answer the question about what is happening in the video.\n",
    "\n",
    "Transcription: <TRANSCRIPTION_OUTPUT>\n",
    "\n",
    "Question: Is the subject coherent in what they are speaking? Are they delivering an easy to understand story, or are they deterring a lot from the central topic and delivering an unorganized speech?\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Comp_task_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Analyze the provided text transcription of the person’s speech and answer the question about what is happening in the video.\n",
    "\n",
    "Transcription: <TRANSCRIPTION_OUTPUT>\n",
    "\n",
    "Question: Please indicate whether the subject was able to follow the instructions while completing the task. If the subject was doing something differently, please describe.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Lip_parting_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate the extent of lips parting when the subject is not saying anything (i.e., always/most of the times/sometimes/very few times/never).\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Masked_faceExp_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate which of the following are true for the subject: (i) The individual's face appears blank and emotionless, even when they are trying to express an emotion. (ii) The expression is weak or asymmetrical, and the individual has difficulty holding an expression (e.g., smile) for an extended period. Also mention if you observe other facial expression abnormalities.\n",
    "\n",
    "Answer: \n",
    "\n",
    "'''\n",
    "\n",
    "Observations_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Document any abnormal signs observed in body parts other than the face. This includes, but is not limited to, tremors in the hands, involuntary shaking or rhythmic movements of the upper or lower extremities, stiffness or rigidity in the limbs, reduced arm swing while speaking, or any signs of bradykinesia (slowness of movement). Additionally, note any abnormal postures, difficulty in maintaining balance, or other motor irregularities that may be indicative of Parkinson’s disease.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Overall_app_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Provide a brief description of the subject's perceived state of mind, noting whether they appear energetic, exhausted, calm, confused, or exhibit any other relevant emotional or cognitive cues.\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Other_people_present_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate whether any other individuals were present in the background. If so, provide a brief description (e.g., \"An older male is visible in the background\"). Conclude with a final answer: \"Yes\" or \"No\".\n",
    "\n",
    "Answer: \n",
    " \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Comp_sent_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease.\n",
    "\n",
    "Analyze the provided text transcription of the person’s speech and answer the question about what is happening in the video.\n",
    "\n",
    "Transcription: <TRANSCRIPTION_OUTPUT>\n",
    "\n",
    "\n",
    "Question: Indicate whether the subject is using complex sentences that are difficult to understand. Conclude with a final answer: \"Yes\" or \"No\".\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "Other_BP_visible_query = '''\n",
    "\n",
    "Imagine you are a clinician specializing in movement disorders. Rely on your knowledge of neurology and clinical care. Now, you are watching a home-recorded video of a person performing some tasks used to assess Parkinson's disease. No experts supervise the person, so there can be different types of noise, or the person may not follow the task instructions properly. The person can also show symptoms that may be associated with having Parkinson's disease. Focus on the noises, task instructions, user compliance, and possible symptoms of Parkinson's disease while answering the question.\n",
    "\n",
    "Task instructions: The person will talk about a recent book they have read or a movie or TV show they have watched. The person will speak for approximately one minute. They should be front-facing the camera, and their face must be visible in the recording frame. There should not be any other person visible in the recording frame. The background should not be dark or overlit and should have good contrast against the person's face. For this task, the face is the most crucial body part you should focus on. However, you should also observe other body parts for relevant symptoms or signs of Parkinson's disease. \n",
    "\n",
    "Answer the question about what is happening in the video.\n",
    "\n",
    "\n",
    "Question: Indicate if any body part critical for this task is partially visible (or invisible). For example, was the subject wearing a mask that may have obstructed important visual information? Or did the face go out of frame while the subject was completing the task?\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_file</th>\n",
       "      <th>Q_ID</th>\n",
       "      <th>Prompt_name</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Cayla_Resp</th>\n",
       "      <th>Nami_Resp</th>\n",
       "      <th>Natalia_Resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Video_file, Q_ID, Prompt_name, Model_Name, Cayla_Resp, Nami_Resp, Natalia_Resp]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Setup file paths ===\n",
    "cayla_files = \"/localdisk4/panwla/Projects/park_vlm/Annotations/Cayla/cayla_df_with_transcriptions.csv\"\n",
    "nami_files = \"/localdisk4/panwla/Projects/park_vlm/Annotations/Clinical/Nami/nami_common_final.csv\"\n",
    "natalia_files = \"/localdisk4/panwla/Projects/park_vlm/Annotations/Clinical/Natalia/natalia_common_final.csv\"\n",
    "\n",
    "video_path = \"/localdisk4/panwla/Projects/park_vlm/Videos/Videos\"\n",
    "\n",
    "# === Load video references ===\n",
    "cayla_df = pd.read_csv(cayla_files)\n",
    "nami_df = pd.read_csv(nami_files)\n",
    "natalia_df = pd.read_csv(natalia_files)\n",
    "\n",
    "\n",
    "\n",
    "# Identify prompt + video columns\n",
    "prompt_columns = list(cayla_df.columns[0:13])  # first 13 columns\n",
    "video_column = ['video']\n",
    "\n",
    "# Subset Nami and Natalia using same columns\n",
    "nami_df = nami_df.loc[:, prompt_columns + video_column]\n",
    "natalia_df = natalia_df.loc[:, prompt_columns + video_column]\n",
    "\n",
    "# List available videos\n",
    "available_videos = os.listdir(video_path)\n",
    "\n",
    "# === Empty DataFrame to collect results ===\n",
    "columns = [\"Video_file\", \"Q_ID\", \"Prompt_name\", \"Model_Name\", \"Cayla_Resp\", \"Nami_Resp\", \"Natalia_Resp\"]\n",
    "output = pd.DataFrame(columns=columns)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Apparent difficulty completing the task', 'Background and lighting',\n",
       "       'Blink rate', 'Camera Position', 'Coherence',\n",
       "       'Compliance with tasks instructions',\n",
       "       'Lips parting when the mouth is at rest', 'Masked facies',\n",
       "       'Observations of other body parts not being directly assessed',\n",
       "       'Overall appearance', 'Presence of other persons',\n",
       "       'Usage of complex sentence', 'Visibility of significant body parts',\n",
       "       'video', 'Transcription'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cayla_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Store prompts in a dictionary ===\n",
    "prompt_dict = {\n",
    "    \"facial_expressions_query\": facial_expressions_query,\n",
    "    \"BG_and_lighting_query\": BG_and_lighting_query,\n",
    "    \"Blink_rate_query\": Blink_rate_query,\n",
    "    \"Camera_position_query\": Camera_position_query,\n",
    "    \"Coherence_query\": Coherence_query,\n",
    "    \"Comp_task_query\": Comp_task_query,\n",
    "    \"Lip_parting_query\": Lip_parting_query,\n",
    "    \"Masked_faceExp_query\": Masked_faceExp_query,\n",
    "    \"Observations_query\": Observations_query,\n",
    "    \"Overall_app_query\": Overall_app_query,\n",
    "    \"Other_people_present_query\": Other_people_present_query,\n",
    "    \"Other_BP_visible_query\": Other_BP_visible_query,\n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B:\n",
      "- configuration_videollama3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VideoLLaMA3 model: DAMO-NLP-SG/VideoLLaMA3-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B:\n",
      "- modeling_videollama3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164cc5a285324801bb6f565810ca4dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8527198c6e8c4e6aa586667ecce21337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B:\n",
      "- processing_videollama3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoLLaMA3 model loaded successfully\n",
      "Processing batch 1 (rows 0 to 0)\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "🔄 Modified prompt Coherence_query with transcription\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "🔄 Modified prompt Comp_task_query with transcription\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "⚠️ Error analyzing video with VideoLLaMA3: expand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"\n",
      "Processing batch 2 (rows 1 to 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 212\u001b[0m\n\u001b[1;32m    207\u001b[0m model_components \u001b[38;5;241m=\u001b[39m load_videollama3_model(\n\u001b[1;32m    208\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDAMO-NLP-SG/VideoLLaMA3-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Process all prompts using VideoLLaMA3\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_prompts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcayla_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnami_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnami_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnatalia_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnatalia_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Process one video at a time to optimize memory usage\u001b[39;49;00m\n\u001b[1;32m    220\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Save the final results\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# results_df.to_csv(\"videollama3_results_final.csv\", index=False)\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis complete! Results saved to videollama3_results_final.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 165\u001b[0m, in \u001b[0;36mprocess_all_prompts\u001b[0;34m(input_df, prompt_dict, video_path, model_components, nami_df, natalia_df, batch_size)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Process video using VideoLLaMA3.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_video_with_videollama3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_video_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessed_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can adjust this value as needed\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# device=\"cuda\"\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m💥 Critical error processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, attempting recovery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 82\u001b[0m, in \u001b[0;36manalyze_video_with_videollama3\u001b[0;34m(video_path, prompt_text, model, processor, max_new_tokens, fps, max_frames)\u001b[0m\n\u001b[1;32m     70\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     71\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     72\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     },\n\u001b[1;32m     79\u001b[0m ]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Process the conversation to create inputs for the model.\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(first_device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m     84\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(v) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m     85\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n",
      "File \u001b[0;32m/localdisk4/panwla/huggingface_cache/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-7B/a498675483e2be8e98d092a2cb11a608c2caa8dd/processing_videollama3.py:708\u001b[0m, in \u001b[0;36mVideollama3Qwen2Processor.__call__\u001b[0;34m(self, text, conversation, images, return_labels, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot provide \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_plain(text, images, return_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/localdisk4/panwla/huggingface_cache/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-7B/a498675483e2be8e98d092a2cb11a608c2caa8dd/processing_videollama3.py:614\u001b[0m, in \u001b[0;36mVideollama3Qwen2Processor._process_conversation\u001b[0;34m(self, conversation, images, return_labels, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversation must be a list of messages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 614\u001b[0m     conversation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_multimodal_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_multimodal_data(conversation)\n\u001b[1;32m    617\u001b[0m output_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_kwargs(\n\u001b[1;32m    618\u001b[0m     Videollama3Qwen2ProcessorKwargs,\n\u001b[1;32m    619\u001b[0m     tokenizer_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39minit_kwargs,\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    621\u001b[0m )\n",
      "File \u001b[0;32m/localdisk4/panwla/huggingface_cache/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-7B/a498675483e2be8e98d092a2cb11a608c2caa8dd/processing_videollama3.py:473\u001b[0m, in \u001b[0;36mVideollama3Qwen2Processor._load_multimodal_data\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    472\u001b[0m     load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m end_time\n\u001b[0;32m--> 473\u001b[0m images, timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m content, start_time, end_time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(contents, start_times, end_times):\n\u001b[1;32m    476\u001b[0m     cur_images, cur_timestamps \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/localdisk4/panwla/huggingface_cache/modules/transformers_modules/DAMO-NLP-SG/VideoLLaMA3-7B/a498675483e2be8e98d092a2cb11a608c2caa8dd/processing_videollama3.py:402\u001b[0m, in \u001b[0;36mVideollama3Qwen2Processor.load_video\u001b[0;34m(self, video_path, start_time, end_time, fps, max_frames, size, size_divisible, precise_time, verbose, temporal_factor)\u001b[0m\n\u001b[1;32m    400\u001b[0m     stream \u001b[38;5;241m=\u001b[39m ffmpeg\u001b[38;5;241m.\u001b[39mfilter(stream, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m, new_w, new_h)\n\u001b[1;32m    401\u001b[0m stream \u001b[38;5;241m=\u001b[39m ffmpeg\u001b[38;5;241m.\u001b[39moutput(stream, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipe:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m, pix_fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_kwargs)\n\u001b[0;32m--> 402\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_stdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out, np\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, new_h, new_w, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mtranspose([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/site-packages/ffmpeg/_run.py:322\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ffmpeg for the supplied node graph.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mReturns: (out, err) tuple containing captured stdout and stderr data.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m process \u001b[38;5;241m=\u001b[39m run_async(\n\u001b[1;32m    314\u001b[0m     stream_spec,\n\u001b[1;32m    315\u001b[0m     cmd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     overwrite_output\u001b[38;5;241m=\u001b[39moverwrite_output,\n\u001b[1;32m    321\u001b[0m )\n\u001b[0;32m--> 322\u001b[0m out, err \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retcode:\n",
      "File \u001b[0;32m/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/subprocess.py:2021\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2015\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2016\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2021\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/localdisk4/panwla/conda_envs/env3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import warnings\n",
    "from PIL import Image\n",
    "\n",
    "# Import Hugging Face Transformers for VideoLLaMA3\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Set PyTorch memory allocation configuration\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def load_videollama3_model(model_name=\"DAMO-NLP-SG/VideoLLaMA3-7B\", \n",
    "                        #    device=\"cuda\"\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Load the VideoLLaMA3 model and processor from Hugging Face.\n",
    "    \"\"\"\n",
    "    print(f\"Loading VideoLLaMA3 model: {model_name}\")\n",
    "    # Clean up any previous CUDA usage\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "         model_name,\n",
    "         trust_remote_code=True,\n",
    "         device_map=None,\n",
    "         torch_dtype=torch.bfloat16,\n",
    "         attn_implementation=\"flash_attention_2\",\n",
    "    ).to(\"cuda\")  \n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model.eval()\n",
    "    print(\"VideoLLaMA3 model loaded successfully\")\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def analyze_video_with_videollama3(\n",
    "    video_path, \n",
    "    prompt_text, \n",
    "    model,\n",
    "    processor, \n",
    "    max_new_tokens=128, \n",
    "    fps=1, \n",
    "    max_frames=128, \n",
    "    # device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a video using VideoLLaMA3 by constructing a conversation input that includes the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clear GPU cache before processing\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # pick device the model lives on\n",
    "        if hasattr(model, \"hf_device_map\"):\n",
    "            first_device = next(iter(model.hf_device_map.values()))\n",
    "        else:\n",
    "            first_device = next(model.parameters()).device     # works in single‑GPU mode\n",
    "\n",
    "        \n",
    "        # Construct the conversation input with video and text components.\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"video\", \"video\": {\"video_path\": video_path, \"fps\": fps, \"max_frames\": max_frames}},\n",
    "                    {\"type\": \"text\", \"text\": prompt_text},\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        # Process the conversation to create inputs for the model.\n",
    "        inputs = processor(conversation=conversation, return_tensors=\"pt\")\n",
    "\n",
    "        # move everything to the model’s device without changing dtype\n",
    "        inputs = {k: (v.to(first_device) if torch.is_tensor(v) else v)\n",
    "                for k, v in inputs.items()}\n",
    "\n",
    "        # now, *optionally* down‑cast only the heavy video tensor\n",
    "        if \"pixel_values\" in inputs:\n",
    "            inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        \n",
    "        response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        # Clear GPU memory after processing\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error analyzing video with VideoLLaMA3: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "\n",
    "def process_all_prompts(input_df, prompt_dict, video_path, model_components, nami_df=None, natalia_df=None, batch_size=1):\n",
    "    \"\"\"\n",
    "    Process the list of prompts on the provided videos using the VideoLLaMA3 model.\n",
    "    This function searches for video files in the provided folder, applies any prompt-specific\n",
    "    modifications (e.g. inserting a transcription), and then processes each video for all prompts.\n",
    "    \"\"\"\n",
    "    model, processor = model_components\n",
    "    available_videos = [f for f in os.listdir(video_path) if f.endswith(\".mp4\")]\n",
    "    \n",
    "    # List of prompts that need transcription insertion.\n",
    "    transcription_prompts = [\"Coherence_query\", \"Comp_task_query\", \"Comp_sent_query\"]\n",
    "    \n",
    "    # Define output columns for the results DataFrame.\n",
    "    columns = [\"Video_file\", \"Q_ID\", \"Prompt_name\", \"Model_Name\", \"Output\", \"Cayla_Resp\", \"Nami_Resp\", \"Natalia_Resp\"]\n",
    "    output = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # Process videos in batches to manage memory.\n",
    "    for batch_start in range(0, len(input_df), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(input_df))\n",
    "        print(f\"Processing batch {batch_start//batch_size + 1} (rows {batch_start} to {batch_end-1})\")\n",
    "        \n",
    "        for idx, row in input_df.iloc[batch_start:batch_end].iterrows():\n",
    "            video_name = row.get(\"video\", None)\n",
    "            if not isinstance(video_name, str) or not video_name:\n",
    "                continue\n",
    "                \n",
    "            transcription = row.get(\"Transcription\", \"\")\n",
    "            # Locate matching video file in the folder.\n",
    "            matched_file = next((f for f in available_videos\n",
    "                                 if video_name[-25:] in f), None)\n",
    "        \n",
    "            if not matched_file:\n",
    "                print(f\"❌ No matching video for {video_name}\")\n",
    "                continue\n",
    "\n",
    "            full_video_path = os.path.join(video_path, matched_file)\n",
    "            prompt_columns = list(input_df.columns[:13])\n",
    "            \n",
    "            for q_id, (prompt_name, prompt_text) in enumerate(prompt_dict.items(), start=1):\n",
    "                processed_prompt = prompt_text\n",
    "                if prompt_name in transcription_prompts and transcription:\n",
    "                    processed_prompt = processed_prompt.replace(\"<TRANSCRIPTION_OUTPUT>\", transcription)\n",
    "                    print(f\"🔄 Modified prompt {prompt_name} with transcription\")\n",
    "                \n",
    "                # Retrieve previous responses if available.\n",
    "                cayla_resp = row[prompt_columns[q_id - 1]] if q_id - 1 < len(prompt_columns) else \"\"\n",
    "                \n",
    "                nami_resp = \"\"\n",
    "                if nami_df is not None:\n",
    "                    nami_row = nami_df[nami_df[\"video\"] == video_name]\n",
    "                    if not nami_row.empty and q_id - 1 < len(prompt_columns):\n",
    "                        nami_resp = nami_row[prompt_columns[q_id - 1]].values[0]\n",
    "                \n",
    "                natalia_resp = \"\"\n",
    "                if natalia_df is not None:\n",
    "                    natalia_row = natalia_df[natalia_df[\"video\"] == video_name]\n",
    "                    if not natalia_row.empty and q_id - 1 < len(prompt_columns):\n",
    "                        natalia_resp = natalia_row[prompt_columns[q_id - 1]].values[0]\n",
    "                \n",
    "                # Process video using VideoLLaMA3.\n",
    "                try:\n",
    "                    response = analyze_video_with_videollama3(\n",
    "                        full_video_path,\n",
    "                        processed_prompt,\n",
    "                        model,\n",
    "                        processor,\n",
    "                        max_new_tokens=128,  # You can adjust this value as needed\n",
    "                        fps=1,\n",
    "                        max_frames=128,\n",
    "                        # device=\"cuda\"\n",
    "                    )\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"💥 Critical error processing {prompt_name}, attempting recovery: {e}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    \n",
    "                # Append results to the output DataFrame.\n",
    "                new_row = {\n",
    "                    \"Video_file\": matched_file,\n",
    "                    \"Q_ID\": q_id,\n",
    "                    \"Prompt_name\": prompt_name,\n",
    "                    \"Model_Name\": \"VideoLLaMA3\",\n",
    "                    \"Output\": response,\n",
    "                    \"Cayla_Resp\": cayla_resp,\n",
    "                    \"Nami_Resp\": nami_resp,\n",
    "                    \"Natalia_Resp\": natalia_resp\n",
    "                }\n",
    "                \n",
    "                output = pd.concat([output, pd.DataFrame([new_row])], ignore_index=True)\n",
    "                output.to_csv(\"videollama3_results_partial.csv\", index=False)\n",
    "            \n",
    "            # Clean GPU memory after processing each video.\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Save intermediate results after each batch.\n",
    "        output.to_csv(f\"videollama3_results_batch_{batch_start//batch_size + 1}.csv\", index=False)\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "# Load VideoLLaMA3 model and processor\n",
    "model_components = load_videollama3_model(\n",
    "    model_name=\"DAMO-NLP-SG/VideoLLaMA3-7B\",\n",
    ")\n",
    "\n",
    "# Process all prompts using VideoLLaMA3\n",
    "results_df = process_all_prompts(\n",
    "    input_df=cayla_df,\n",
    "    prompt_dict=prompt_dict,\n",
    "    video_path=video_path,\n",
    "    model_components=model_components,\n",
    "    nami_df=nami_df,\n",
    "    natalia_df=natalia_df,\n",
    "    batch_size=1  # Process one video at a time to optimize memory usage\n",
    ")\n",
    "\n",
    "# Save the final results\n",
    "# results_df.to_csv(\"videollama3_results_final.csv\", index=False)\n",
    "print(\"Analysis complete! Results saved to videollama3_results_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
