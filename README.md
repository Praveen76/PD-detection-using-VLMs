# Parkinson's Disease Detection using Vision-Language Models (VLMs)

This project explores the use of pretrained Vision-Language Models (VLMs) for **zero-shot Parkinson's Disease (PD) detection** from clinical video data. By leveraging multimodal prompts and structured visual features, the system evaluates motor symptoms across 325+ patient videosâ€”without requiring any model fine-tuning.

## ğŸ“Œ Highlights

- **Zero-Shot Evaluation** using 12 carefully designed diagnostic prompts.
- **325+ Clinical Videos** covering various PD-related motor patterns.
- **Modular Pipeline** for video selection, embedding extraction, surrogate feature computation, and evaluation.
- Built with **PyTorch**, **OpenCLIP**, and **DeepSpeed** for scalable inference.

---

## ğŸ“ Project Structure

```

â”œâ”€â”€ Annotations/              # Label files and structured QA data
â”œâ”€â”€ Dataset/                  # Raw and processed video files
â”œâ”€â”€ Download\_Voxceleb/        # Utility to fetch VoxCeleb data (if needed)
â”œâ”€â”€ EmbeddingEvaluation/      # Evaluation scripts for single/multi-view embeddings
â”‚   â””â”€â”€ SingleViewEmbedding/
â”œâ”€â”€ Experiments/              # All experiments using different VLMs
â”œâ”€â”€ Metadata/                 # Patient meta-info and clinical context
â”œâ”€â”€ SmileFeatures/            # Smile-related feature extraction
â”‚   â””â”€â”€ Evaluation/
â”œâ”€â”€ SurrogateFeatures/        # Temporal and spatial surrogate metrics (e.g., velocity, angle)
â”œâ”€â”€ Utils/                    # Common utilities
â”œâ”€â”€ VideoEmbeddings/          # Precomputed CLIP/VL embeddings
â”œâ”€â”€ VideoSelection/           # Filtering and selection logic
â”œâ”€â”€ videoSamples/ViViT\_v02/   # ViViT-style samples for benchmarking
â”œâ”€â”€ practice/                 # Temporary experiments / notebooks
â”œâ”€â”€ remaining\_annotations.txt # Remaining manual label entries
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pyrightconfig.json
â””â”€â”€ requirements.txt          # Environment dependencies

````

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/Praveen76/PD-detection-using-VLMs.git
cd PD-detection-using-VLMs
````

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up Data & Embeddings

Place video data under `Dataset/`, annotations under `Annotations/`, and ensure embeddings are generated or loaded into `VideoEmbeddings/`.

---

## ğŸ“Š Evaluation

Run the evaluation using the zero-shot prompts defined in the pipeline:

```bash
python EmbeddingEvaluation/SingleViewEmbedding/eval_pipeline.py
```

Prompts are encoded using CLIP, and scores are aggregated over frames and prompts.

---

## ğŸ§  Model & Approach

* Uses **pretrained CLIP-based VLMs** for visual-textual embedding.
* Applies **12 structured prompts** targeting PD symptoms like tremor, rigidity, and facial expression.
* Embedding similarity used as proxy for class confidence.

---

## ğŸ“œ License

This project is licensed under the MIT License. See `LICENSE` for details.
now if you'd like to include figures, paper links, or citation blocks. I can adapt it to academic, demo, or production tone as needed.

